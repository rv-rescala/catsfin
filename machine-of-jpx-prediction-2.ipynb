{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T23:56:22.598179Z","iopub.execute_input":"2022-04-21T23:56:22.599378Z","iopub.status.idle":"2022-04-21T23:56:22.607029Z","shell.execute_reply.started":"2022-04-21T23:56:22.599314Z","shell.execute_reply":"2022-04-21T23:56:22.605831Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# I/O Func\nBASE_PATH = Path(f'/kaggle/working')\n\ndef adjusting_price(price, key: str):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[f\"CumulativeAdjustmentFactor{key}\"] * df[key]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef adjusting_volume(price, key = \"Volume\"):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[key] / df[f\"CumulativeAdjustmentFactor{key}\"]  \n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef read_prices(dir_name: str, securities_code: int = None):\n    \"\"\"[Important: the dateset of 2020/10/1 is lost because of system failer in JPX, see: https://www.jpx.co.jp/corporate/news/news-releases/0060/20201019-01.html]\n    \n    \"\"\"\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    df = pd.read_csv(base_path / 'stock_prices.csv')\n    df.loc[: ,\"Date\"] = pd.to_datetime(df.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n    df = df[df['Open'].notna()]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_stock_list(securities_code: int = None, only_universe: bool = True):\n    df = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\n    df.loc[: ,\"EffectiveDate\"] = pd.to_datetime(df.loc[: ,\"EffectiveDate\"], format=\"%Y%m%d\")\n    if only_universe:\n        df = df[df['Universe0']]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef merge_data(prices, stock_list):\n    # stock_prices がベース\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n    \n    return base_df\n\ndef read_train_data_by_price(securities_code: int = None, with_supplemental: bool = True):\n    \"\"\"[The train base is price dataset, the other data are joined to prices DF by left join]\n    \n    \"\"\"\n    # origin\n    df = merge_data(prices=read_prices(dir_name=\"train_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n    \n    # supplyment\n    if with_supplemental:\n        supplemental_df = merge_data(prices=read_prices(dir_name=\"supplemental_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n        df = pd.concat([df, supplemental_df]).reset_index(drop=True)\n        \n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef collector(prices, options, financials, trades, secondary_prices, stock_list):\n    # 読み込んだデータを統合して一つのファイルに纏める\n    df = merge_data(prices, stock_list)\n    # AdjustedClose項目の生成\n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef write_df(df, filename):\n    df.to_csv(BASE_PATH / f'{filename}.csv',index = False)\n    \nimport joblib\ndef write_model(model, name):\n    # save model\n    joblib.dump(model, f'{BASE_PATH}/{name}.pkl')\n\n\n# load model\ndef read_model(name):\n    return joblib.load(f'{BASE_PATH}/{name}.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:59:32.984598Z","iopub.execute_input":"2022-04-22T01:59:32.984920Z","iopub.status.idle":"2022-04-22T01:59:33.019542Z","shell.execute_reply.started":"2022-04-22T01:59:32.984890Z","shell.execute_reply":"2022-04-22T01:59:33.018838Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train_df = read_train_data_by_price()\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-04-21T23:56:22.649187Z","iopub.execute_input":"2022-04-21T23:56:22.649425Z","iopub.status.idle":"2022-04-21T23:58:38.515942Z","shell.execute_reply.started":"2022-04-21T23:56:22.649399Z","shell.execute_reply":"2022-04-21T23:58:38.514765Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                 RowId       Date  SecuritiesCode    Open    High     Low  \\\n0        20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0   \n1        20170105_1301 2017-01-05            1301  2743.0  2747.0  2735.0   \n2        20170106_1301 2017-01-06            1301  2734.0  2744.0  2720.0   \n3        20170110_1301 2017-01-10            1301  2745.0  2754.0  2735.0   \n4        20170111_1301 2017-01-11            1301  2748.0  2752.0  2737.0   \n...                ...        ...             ...     ...     ...     ...   \n2436634  20220221_9997 2022-02-21            9997   725.0   729.0   719.0   \n2436635  20220222_9997 2022-02-22            9997   719.0   723.0   711.0   \n2436636  20220224_9997 2022-02-24            9997   709.0   725.0   708.0   \n2436637  20220225_9997 2022-02-25            9997   725.0   738.0   724.0   \n2436638  20220228_9997 2022-02-28            9997   731.0   737.0   726.0   \n\n          Close  Volume  AdjustmentFactor  ExpectedDividend  ...  \\\n0        2742.0   31400               1.0               NaN  ...   \n1        2738.0   17900               1.0               NaN  ...   \n2        2740.0   19900               1.0               NaN  ...   \n3        2748.0   24200               1.0               NaN  ...   \n4        2745.0    9300               1.0               NaN  ...   \n...         ...     ...               ...               ...  ...   \n2436634   727.0  116400               1.0               NaN  ...   \n2436635   721.0  225500               1.0               NaN  ...   \n2436636   719.0  195600               1.0               NaN  ...   \n2436637   733.0  170500               1.0               NaN  ...   \n2436638   734.0  288100               1.0               NaN  ...   \n\n         CumulativeAdjustmentFactorClose  AdjustedClose  \\\n0                                    1.0         2742.0   \n1                                    1.0         2738.0   \n2                                    1.0         2740.0   \n3                                    1.0         2748.0   \n4                                    1.0         2745.0   \n...                                  ...            ...   \n2436634                              1.0          727.0   \n2436635                              1.0          721.0   \n2436636                              1.0          719.0   \n2436637                              1.0          733.0   \n2436638                              1.0          734.0   \n\n        CumulativeAdjustmentFactorOpen AdjustedOpen  \\\n0                                  1.0       2734.0   \n1                                  1.0       2743.0   \n2                                  1.0       2734.0   \n3                                  1.0       2745.0   \n4                                  1.0       2748.0   \n...                                ...          ...   \n2436634                            1.0        725.0   \n2436635                            1.0        719.0   \n2436636                            1.0        709.0   \n2436637                            1.0        725.0   \n2436638                            1.0        731.0   \n\n        CumulativeAdjustmentFactorHigh AdjustedHigh  \\\n0                                  1.0       2755.0   \n1                                  1.0       2747.0   \n2                                  1.0       2744.0   \n3                                  1.0       2754.0   \n4                                  1.0       2752.0   \n...                                ...          ...   \n2436634                            1.0        729.0   \n2436635                            1.0        723.0   \n2436636                            1.0        725.0   \n2436637                            1.0        738.0   \n2436638                            1.0        737.0   \n\n        CumulativeAdjustmentFactorLow AdjustedLow  \\\n0                                 1.0      2730.0   \n1                                 1.0      2735.0   \n2                                 1.0      2720.0   \n3                                 1.0      2735.0   \n4                                 1.0      2737.0   \n...                               ...         ...   \n2436634                           1.0       719.0   \n2436635                           1.0       711.0   \n2436636                           1.0       708.0   \n2436637                           1.0       724.0   \n2436638                           1.0       726.0   \n\n        CumulativeAdjustmentFactorVolume AdjustedVolume  \n0                                    1.0        31400.0  \n1                                    1.0        17900.0  \n2                                    1.0        19900.0  \n3                                    1.0        24200.0  \n4                                    1.0         9300.0  \n...                                  ...            ...  \n2436634                              1.0       116400.0  \n2436635                              1.0       225500.0  \n2436636                              1.0       195600.0  \n2436637                              1.0       170500.0  \n2436638                              1.0       288100.0  \n\n[2436639 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowId</th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>...</th>\n      <th>CumulativeAdjustmentFactorClose</th>\n      <th>AdjustedClose</th>\n      <th>CumulativeAdjustmentFactorOpen</th>\n      <th>AdjustedOpen</th>\n      <th>CumulativeAdjustmentFactorHigh</th>\n      <th>AdjustedHigh</th>\n      <th>CumulativeAdjustmentFactorLow</th>\n      <th>AdjustedLow</th>\n      <th>CumulativeAdjustmentFactorVolume</th>\n      <th>AdjustedVolume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20170104_1301</td>\n      <td>2017-01-04</td>\n      <td>1301</td>\n      <td>2734.0</td>\n      <td>2755.0</td>\n      <td>2730.0</td>\n      <td>2742.0</td>\n      <td>31400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2742.0</td>\n      <td>1.0</td>\n      <td>2734.0</td>\n      <td>1.0</td>\n      <td>2755.0</td>\n      <td>1.0</td>\n      <td>2730.0</td>\n      <td>1.0</td>\n      <td>31400.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20170105_1301</td>\n      <td>2017-01-05</td>\n      <td>1301</td>\n      <td>2743.0</td>\n      <td>2747.0</td>\n      <td>2735.0</td>\n      <td>2738.0</td>\n      <td>17900</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2738.0</td>\n      <td>1.0</td>\n      <td>2743.0</td>\n      <td>1.0</td>\n      <td>2747.0</td>\n      <td>1.0</td>\n      <td>2735.0</td>\n      <td>1.0</td>\n      <td>17900.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20170106_1301</td>\n      <td>2017-01-06</td>\n      <td>1301</td>\n      <td>2734.0</td>\n      <td>2744.0</td>\n      <td>2720.0</td>\n      <td>2740.0</td>\n      <td>19900</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2740.0</td>\n      <td>1.0</td>\n      <td>2734.0</td>\n      <td>1.0</td>\n      <td>2744.0</td>\n      <td>1.0</td>\n      <td>2720.0</td>\n      <td>1.0</td>\n      <td>19900.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20170110_1301</td>\n      <td>2017-01-10</td>\n      <td>1301</td>\n      <td>2745.0</td>\n      <td>2754.0</td>\n      <td>2735.0</td>\n      <td>2748.0</td>\n      <td>24200</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2748.0</td>\n      <td>1.0</td>\n      <td>2745.0</td>\n      <td>1.0</td>\n      <td>2754.0</td>\n      <td>1.0</td>\n      <td>2735.0</td>\n      <td>1.0</td>\n      <td>24200.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20170111_1301</td>\n      <td>2017-01-11</td>\n      <td>1301</td>\n      <td>2748.0</td>\n      <td>2752.0</td>\n      <td>2737.0</td>\n      <td>2745.0</td>\n      <td>9300</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2745.0</td>\n      <td>1.0</td>\n      <td>2748.0</td>\n      <td>1.0</td>\n      <td>2752.0</td>\n      <td>1.0</td>\n      <td>2737.0</td>\n      <td>1.0</td>\n      <td>9300.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2436634</th>\n      <td>20220221_9997</td>\n      <td>2022-02-21</td>\n      <td>9997</td>\n      <td>725.0</td>\n      <td>729.0</td>\n      <td>719.0</td>\n      <td>727.0</td>\n      <td>116400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>727.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>729.0</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>116400.0</td>\n    </tr>\n    <tr>\n      <th>2436635</th>\n      <td>20220222_9997</td>\n      <td>2022-02-22</td>\n      <td>9997</td>\n      <td>719.0</td>\n      <td>723.0</td>\n      <td>711.0</td>\n      <td>721.0</td>\n      <td>225500</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>721.0</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>723.0</td>\n      <td>1.0</td>\n      <td>711.0</td>\n      <td>1.0</td>\n      <td>225500.0</td>\n    </tr>\n    <tr>\n      <th>2436636</th>\n      <td>20220224_9997</td>\n      <td>2022-02-24</td>\n      <td>9997</td>\n      <td>709.0</td>\n      <td>725.0</td>\n      <td>708.0</td>\n      <td>719.0</td>\n      <td>195600</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>709.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>708.0</td>\n      <td>1.0</td>\n      <td>195600.0</td>\n    </tr>\n    <tr>\n      <th>2436637</th>\n      <td>20220225_9997</td>\n      <td>2022-02-25</td>\n      <td>9997</td>\n      <td>725.0</td>\n      <td>738.0</td>\n      <td>724.0</td>\n      <td>733.0</td>\n      <td>170500</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>733.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>738.0</td>\n      <td>1.0</td>\n      <td>724.0</td>\n      <td>1.0</td>\n      <td>170500.0</td>\n    </tr>\n    <tr>\n      <th>2436638</th>\n      <td>20220228_9997</td>\n      <td>2022-02-28</td>\n      <td>9997</td>\n      <td>731.0</td>\n      <td>737.0</td>\n      <td>726.0</td>\n      <td>734.0</td>\n      <td>288100</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>734.0</td>\n      <td>1.0</td>\n      <td>731.0</td>\n      <td>1.0</td>\n      <td>737.0</td>\n      <td>1.0</td>\n      <td>726.0</td>\n      <td>1.0</td>\n      <td>288100.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2436639 rows × 37 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Featrue","metadata":{}},{"cell_type":"code","source":"def cal_moving_average(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"MovingAverage{key}{period}\"\n            col_gap = f\"{col}GapPercent\"\n            df[col] = df[key].rolling(period, min_periods=1).mean()\n            df[col_gap] = (df[key] / df[col]) * 100.0\n        return df\n    return func\n\ndef cal_changing_ration(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"ChangingRatio{key}{period}\"\n            df[col] = df[key].pct_change(period) * 100\n        return df\n    return func\n\ndef cal_historical_vix(key: str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"HistoricalVIX{key}{period}\"\n            df[col] = np.log(df[key]).diff().rolling(period).std()\n        return df\n    return func\n\ndef add_columns_per_code(df, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    df = df.sort_values([\"SecuritiesCode\", \"Date\"])\n    df = df.groupby(\"SecuritiesCode\").apply(func)\n    df = df.reset_index(drop=True)\n    return df\n\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\ndef generate_features(df):\n    base_df = df.copy()\n    prev_column_names = base_df.columns\n    periods = [5, 25, 75]\n    functions = [\n        cal_moving_average(\"AdjustedClose\", periods),\n        cal_moving_average(\"AdjustedOpen\", periods),\n        cal_moving_average(\"AdjustedHigh\", periods),\n        cal_moving_average(\"AdjustedLow\", periods),\n        cal_moving_average(\"AdjustedVolume\", periods),\n        cal_changing_ration(\"AdjustedClose\", periods),\n        cal_changing_ration(\"AdjustedOpen\", periods),\n        cal_changing_ration(\"AdjustedHigh\", periods),\n        cal_changing_ration(\"AdjustedLow\", periods),\n        cal_changing_ration(\"AdjustedVolume\", periods),\n        cal_historical_vix(\"AdjustedClose\", periods),\n        cal_historical_vix(\"AdjustedOpen\", periods),\n        cal_historical_vix(\"AdjustedHigh\", periods),\n        cal_historical_vix(\"AdjustedLow\", periods),\n        cal_historical_vix(\"AdjustedVolume\", periods)\n    ]\n    \n    base_df = add_columns_per_code(base_df, functions)\n    base_df = add_columns_per_day(base_df)\n    \n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    #feats = feats[feats[\"HistoricalVIXAdjustedClose75\"] != 0]\n    return base_df, add_column_names\n\ndef select_features(feature_df, add_column_names, is_train):\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    numerical_cols = sorted(add_column_names)\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n    label_col = ['Target']\n    feat_cols = numerical_cols + categorical_cols\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    if is_train:\n        feature_df.dropna(inplace=True)\n    else:\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n    return feature_df, feat_cols, label_col\n\ndef preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n\n    return feature_df, feat_cols, label_col\n\nfeature_df, feat_cols, label_col = preprocessor(train_df)\n\n# modelの結果をもとにfeat_colsを上書き\nfeat_cols = ['33SectorCode', 'ChangingRatioAdjustedVolume25', 'diff_rate2', 'MovingAverageAdjustedHigh5GapPercent', 'MovingAverageAdjustedOpen5GapPercent', 'HistoricalVIXAdjustedLow5', 'MovingAverageAdjustedClose5GapPercent', 'HistoricalVIXAdjustedOpen5', 'MovingAverageAdjustedLow25GapPercent', 'ChangingRatioAdjustedVolume5', 'HistoricalVIXAdjustedOpen75', 'HistoricalVIXAdjustedVolume5', 'MovingAverageAdjustedVolume25GapPercent', 'diff_rate1', 'ChangingRatioAdjustedHigh5', 'ChangingRatioAdjustedOpen25', 'HistoricalVIXAdjustedOpen25', 'MovingAverageAdjustedClose25GapPercent', 'MovingAverageAdjustedVolume75GapPercent', 'ChangingRatioAdjustedLow25', 'ChangingRatioAdjustedLow5', 'HistoricalVIXAdjustedHigh75', 'MovingAverageAdjustedLow5GapPercent', 'ChangingRatioAdjustedClose75', 'MovingAverageAdjustedClose75', 'MovingAverageAdjustedClose75GapPercent', 'HistoricalVIXAdjustedVolume75']\nfeat_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-21T23:58:38.518553Z","iopub.execute_input":"2022-04-21T23:58:38.519070Z","iopub.status.idle":"2022-04-22T00:00:44.509012Z","shell.execute_reply.started":"2022-04-21T23:58:38.519016Z","shell.execute_reply":"2022-04-22T00:00:44.508064Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['33SectorCode',\n 'ChangingRatioAdjustedVolume25',\n 'diff_rate2',\n 'MovingAverageAdjustedHigh5GapPercent',\n 'MovingAverageAdjustedOpen5GapPercent',\n 'HistoricalVIXAdjustedLow5',\n 'MovingAverageAdjustedClose5GapPercent',\n 'HistoricalVIXAdjustedOpen5',\n 'MovingAverageAdjustedLow25GapPercent',\n 'ChangingRatioAdjustedVolume5',\n 'HistoricalVIXAdjustedOpen75',\n 'HistoricalVIXAdjustedVolume5',\n 'MovingAverageAdjustedVolume25GapPercent',\n 'diff_rate1',\n 'ChangingRatioAdjustedHigh5',\n 'ChangingRatioAdjustedOpen25',\n 'HistoricalVIXAdjustedOpen25',\n 'MovingAverageAdjustedClose25GapPercent',\n 'MovingAverageAdjustedVolume75GapPercent',\n 'ChangingRatioAdjustedLow25',\n 'ChangingRatioAdjustedLow5',\n 'HistoricalVIXAdjustedHigh75',\n 'MovingAverageAdjustedLow5GapPercent',\n 'ChangingRatioAdjustedClose75',\n 'MovingAverageAdjustedClose75',\n 'MovingAverageAdjustedClose75GapPercent',\n 'HistoricalVIXAdjustedVolume75']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Learning","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio\n\n# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score\n\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n# 学習を実行する関数\ndef trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    scores = []\n    models = []\n    params = []\n    i = 0\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n        \n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        ################################\n        # 学習\n        ################################\n        params = {\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'regression',         # 回帰\n            'metric': 'rmse',                  # 損失（誤差）\n            'learning_rate': 0.01,             # 学習率\n            'lambda_l1': 0.5,                  # L1正則化項の係数\n            'lambda_l2': 0.5,                  # L2正則化項の係数\n            'num_leaves': 10,                  # 最大葉枚数\n            'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n            'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n            'bagging_freq': 5,                 # バギング実施頻度\n            'min_child_samples': 10,           # 葉に含まれる最小データ数\n            'seed': seed                       # シード値\n        } \n \n        lgb_results = {}                       \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=2000,              # 計算回数\n            early_stopping_rounds=100,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 4))\n\n        # loss\n        plt.subplot(1,2,1)\n        loss_train = lgb_results['Train']['rmse']\n        loss_test = lgb_results['Valid']['rmse']   \n        plt.xlabel('Iteration')\n        plt.ylabel('logloss')\n        plt.plot(loss_train, label='train loss')\n        plt.plot(loss_test, label='valid loss')\n        plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        write_df(importance, f\"importance_{i}\")\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n\n        ################################\n        # 評価\n        ################################\n        # 推論\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        # 評価\n        score = evaluator(valid, pred)\n\n        scores.append(score)\n        models.append(model)\n        # save model\n        write_model(model, f'model_{i}.txt')\n        i = i + 1\n        # model = lightgbm.Booster(model_file='lgbr_base.txt')\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2022-04-22T00:00:44.510717Z","iopub.execute_input":"2022-04-22T00:00:44.511042Z","iopub.status.idle":"2022-04-22T00:00:44.540322Z","shell.execute_reply.started":"2022-04-22T00:00:44.511001Z","shell.execute_reply":"2022-04-22T00:00:44.539757Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\nmodels = trainer(feature_df, feat_cols, label_col, fold_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def predictor(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    \n    # 推論\n    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n    print(preds)\n    \n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n    pred = np.array(preds).mean(axis=0)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred\n\ndef eval_predictor(df, feat_cols, models, target_date=['2022-01-05', '2022-01-06']):\n    # 日次で推論・登録\n    results = []\n    for current_date in target_date:\n        past_df = df.copy()\n        print(f\"round: {current_date}\")\n\n        # 未来のデータを削除\n        target_df = past_df[past_df[\"Date\"] == current_date]\n\n        # 推論20\n        target_df[\"pred\"] = predictor(target_df, feat_cols, models, False)\n\n        # 推論結果からRANKを導出し、提出データに反映\n        target_df = add_rank(target_df)\n        #feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n        results.append(target_df)\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:22:05.207619Z","iopub.execute_input":"2022-04-22T03:22:05.207974Z","iopub.status.idle":"2022-04-22T03:22:05.221573Z","shell.execute_reply.started":"2022-04-22T03:22:05.207939Z","shell.execute_reply":"2022-04-22T03:22:05.220069Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"_df = predictor_date(feature_df, feat_cols, models)\npred_df = _df[0]\npred_df = pred_df[[\"SecuritiesCode\", \"pred\", \"Date\", \"Rank\"]]\ntarget_df = _df[1]\ntarget_df = target_df[[\"SecuritiesCode\", \"Target\"]]\nresult_df = pred_df.merge(target_df, on='SecuritiesCode', how=\"left\")\nresult_df\nwrite_df(result_df, \"result_df\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T03:31:25.271609Z","iopub.execute_input":"2022-04-22T03:31:25.272158Z","iopub.status.idle":"2022-04-22T03:31:27.159027Z","shell.execute_reply.started":"2022-04-22T03:31:25.272092Z","shell.execute_reply":"2022-04-22T03:31:27.158259Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"round: 2022-01-05\n[array([0.00072377, 0.00073266, 0.00074257, ..., 0.00073367, 0.00072377,\n       0.00073266]), array([-0.00097971,  0.00076513,  0.0002153 , ..., -0.00094532,\n       -0.00089665,  0.00032605]), array([-0.00024162,  0.00092054,  0.00019895, ..., -0.00037926,\n       -0.00022333,  0.00115825])]\nround: 2022-01-06\n[array([0.00073367, 0.00074257, 0.00073367, ..., 0.00074257, 0.00074257,\n       0.00072872]), array([-6.77370828e-04, -4.35795404e-04, -1.14502060e-03, ...,\n        4.80975262e-04,  6.79229783e-05,  3.24470033e-04]), array([-0.00012713,  0.00031164, -0.00102964, ...,  0.00087621,\n        0.00099555,  0.00196577])]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:59:53.952279Z","iopub.execute_input":"2022-04-22T01:59:53.953042Z","iopub.status.idle":"2022-04-22T01:59:53.987455Z","shell.execute_reply.started":"2022-04-22T01:59:53.953001Z","shell.execute_reply":"2022-04-22T01:59:53.986252Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/2566298185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 時系列APIのロード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjpx_tokyo_market_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjpx_tokyo_market_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0miter_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/competition.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mjpx_tokyo_market_prediction.competition.make_env\u001b[0;34m()\u001b[0m\n","\u001b[0;31mException\u001b[0m: You can only call `make_env()` once."],"ename":"Exception","evalue":"You can only call `make_env()` once.","output_type":"error"}]},{"cell_type":"code","source":"# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = train_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T01:53:28.752499Z","iopub.execute_input":"2022-04-22T01:53:28.753006Z","iopub.status.idle":"2022-04-22T01:53:29.521180Z","shell.execute_reply.started":"2022-04-22T01:53:28.752973Z","shell.execute_reply":"2022-04-22T01:53:29.520052Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# 日次で推論・登録\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n    current_date = prices[\"Date\"].iloc[0]\n    print(f\"count {i}, {current_date}\")\n\n    if i == 0:\n        # リークを防止するため、時系列APIから受け取ったデータより未来のデータを削除\n        past_df = past_df[past_df[\"Date\"] < current_date]\n\n    # リソース確保のため古い履歴を削除\n    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80)).strftime(\"%Y-%m-%d\")\n    past_df = past_df[past_df[\"Date\"] >= threshold]\n    \n    # 時系列APIから受け取ったデータを履歴データに統合\n    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n\n    # 特徴量エンジニアリング\n    feature_df, feat_cols, label_col = preprocessor(past_df, False)\n\n    # 予測対象レコードだけを抽出\n    feature_df = feature_df[feature_df['Date'] == current_date]\n\n    # 推論\n    feature_df[\"pred\"] = predictor(feature_df, feat_cols, models, False)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    feature_df = add_rank(feature_df)\n    write_df(feature_df, f\"result_{i}\")\n    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n    # 結果を登録\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T02:00:01.864198Z","iopub.execute_input":"2022-04-22T02:00:01.865172Z","iopub.status.idle":"2022-04-22T02:00:01.902646Z","shell.execute_reply.started":"2022-04-22T02:00:01.865126Z","shell.execute_reply":"2022-04-22T02:00:01.901472Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/2348017407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 日次で推論・登録\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinancials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrades\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcurrent_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"count {i}, {current_date}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/jpx-tokyo-stock-exchange-prediction/jpx_tokyo_market_prediction/competition.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36miter_test\u001b[0;34m()\u001b[0m\n","\u001b[0;31mException\u001b[0m: You can only iterate over `iter_test()` once."],"ename":"Exception","evalue":"You can only iterate over `iter_test()` once.","output_type":"error"}]}]}