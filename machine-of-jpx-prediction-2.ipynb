{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T15:23:56.439859Z","iopub.execute_input":"2022-04-21T15:23:56.440310Z","iopub.status.idle":"2022-04-21T15:23:56.448267Z","shell.execute_reply.started":"2022-04-21T15:23:56.440258Z","shell.execute_reply":"2022-04-21T15:23:56.447228Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# I/O Func\ndef adjusting_price(price, key: str):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[f\"CumulativeAdjustmentFactor{key}\"] * df[key]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef adjusting_volume(price, key = \"Volume\"):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[key] / df[f\"CumulativeAdjustmentFactor{key}\"]  \n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef read_prices(dir_name: str, securities_code: int = None):\n    \"\"\"[Important: the dateset of 2020/10/1 is lost because of system failer in JPX, see: https://www.jpx.co.jp/corporate/news/news-releases/0060/20201019-01.html]\n    \n    \"\"\"\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    df = pd.read_csv(base_path / 'stock_prices.csv')\n    df.loc[: ,\"Date\"] = pd.to_datetime(df.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n    df = df[df['Open'].notna()]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_stock_list(securities_code: int = None, only_universe: bool = True):\n    df = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\n    df.loc[: ,\"EffectiveDate\"] = pd.to_datetime(df.loc[: ,\"EffectiveDate\"], format=\"%Y%m%d\")\n    if only_universe:\n        df = df[df['Universe0']]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_train_data_by_price(securities_code: int = None, with_supplemental: bool = True):\n    \"\"\"[The train base is price dataset, the other data are joined to prices DF by left join]\n    \n    \"\"\"\n    def merge_data(prices, stock_list):\n        base_df = prices.copy()\n        _stock_list = stock_list.copy()\n        _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n        base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n        return base_df\n    \n    # origin\n    df = merge_data(prices=read_prices(dir_name=\"train_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n    \n    # supplyment\n    if with_supplemental:\n        supplemental_df = merge_data(prices=read_prices(dir_name=\"supplemental_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n        df = pd.concat([df, supplemental_df]).reset_index(drop=True)\n        \n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef write_df(df, filename):\n    base_path = Path(f'/kaggle/working')\n    df.to_csv(base_path / f'{filename}.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:23:56.459430Z","iopub.execute_input":"2022-04-21T15:23:56.460112Z","iopub.status.idle":"2022-04-21T15:23:56.491359Z","shell.execute_reply.started":"2022-04-21T15:23:56.460061Z","shell.execute_reply":"2022-04-21T15:23:56.490274Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_df = read_train_data_by_price()\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:23:56.500768Z","iopub.execute_input":"2022-04-21T15:23:56.501039Z","iopub.status.idle":"2022-04-21T15:26:13.395126Z","shell.execute_reply.started":"2022-04-21T15:23:56.501010Z","shell.execute_reply":"2022-04-21T15:26:13.394241Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"num_leaves, val_score: 0.022207:   0%|          | 0/20 [40:04<?, ?it/s]\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                 RowId       Date  SecuritiesCode    Open    High     Low  \\\n0        20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0   \n1        20170105_1301 2017-01-05            1301  2743.0  2747.0  2735.0   \n2        20170106_1301 2017-01-06            1301  2734.0  2744.0  2720.0   \n3        20170110_1301 2017-01-10            1301  2745.0  2754.0  2735.0   \n4        20170111_1301 2017-01-11            1301  2748.0  2752.0  2737.0   \n...                ...        ...             ...     ...     ...     ...   \n2436634  20220221_9997 2022-02-21            9997   725.0   729.0   719.0   \n2436635  20220222_9997 2022-02-22            9997   719.0   723.0   711.0   \n2436636  20220224_9997 2022-02-24            9997   709.0   725.0   708.0   \n2436637  20220225_9997 2022-02-25            9997   725.0   738.0   724.0   \n2436638  20220228_9997 2022-02-28            9997   731.0   737.0   726.0   \n\n          Close  Volume  AdjustmentFactor  ExpectedDividend  ...  \\\n0        2742.0   31400               1.0               NaN  ...   \n1        2738.0   17900               1.0               NaN  ...   \n2        2740.0   19900               1.0               NaN  ...   \n3        2748.0   24200               1.0               NaN  ...   \n4        2745.0    9300               1.0               NaN  ...   \n...         ...     ...               ...               ...  ...   \n2436634   727.0  116400               1.0               NaN  ...   \n2436635   721.0  225500               1.0               NaN  ...   \n2436636   719.0  195600               1.0               NaN  ...   \n2436637   733.0  170500               1.0               NaN  ...   \n2436638   734.0  288100               1.0               NaN  ...   \n\n         CumulativeAdjustmentFactorClose  AdjustedClose  \\\n0                                    1.0         2742.0   \n1                                    1.0         2738.0   \n2                                    1.0         2740.0   \n3                                    1.0         2748.0   \n4                                    1.0         2745.0   \n...                                  ...            ...   \n2436634                              1.0          727.0   \n2436635                              1.0          721.0   \n2436636                              1.0          719.0   \n2436637                              1.0          733.0   \n2436638                              1.0          734.0   \n\n        CumulativeAdjustmentFactorOpen AdjustedOpen  \\\n0                                  1.0       2734.0   \n1                                  1.0       2743.0   \n2                                  1.0       2734.0   \n3                                  1.0       2745.0   \n4                                  1.0       2748.0   \n...                                ...          ...   \n2436634                            1.0        725.0   \n2436635                            1.0        719.0   \n2436636                            1.0        709.0   \n2436637                            1.0        725.0   \n2436638                            1.0        731.0   \n\n        CumulativeAdjustmentFactorHigh AdjustedHigh  \\\n0                                  1.0       2755.0   \n1                                  1.0       2747.0   \n2                                  1.0       2744.0   \n3                                  1.0       2754.0   \n4                                  1.0       2752.0   \n...                                ...          ...   \n2436634                            1.0        729.0   \n2436635                            1.0        723.0   \n2436636                            1.0        725.0   \n2436637                            1.0        738.0   \n2436638                            1.0        737.0   \n\n        CumulativeAdjustmentFactorLow AdjustedLow  \\\n0                                 1.0      2730.0   \n1                                 1.0      2735.0   \n2                                 1.0      2720.0   \n3                                 1.0      2735.0   \n4                                 1.0      2737.0   \n...                               ...         ...   \n2436634                           1.0       719.0   \n2436635                           1.0       711.0   \n2436636                           1.0       708.0   \n2436637                           1.0       724.0   \n2436638                           1.0       726.0   \n\n        CumulativeAdjustmentFactorVolume AdjustedVolume  \n0                                    1.0        31400.0  \n1                                    1.0        17900.0  \n2                                    1.0        19900.0  \n3                                    1.0        24200.0  \n4                                    1.0         9300.0  \n...                                  ...            ...  \n2436634                              1.0       116400.0  \n2436635                              1.0       225500.0  \n2436636                              1.0       195600.0  \n2436637                              1.0       170500.0  \n2436638                              1.0       288100.0  \n\n[2436639 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowId</th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>...</th>\n      <th>CumulativeAdjustmentFactorClose</th>\n      <th>AdjustedClose</th>\n      <th>CumulativeAdjustmentFactorOpen</th>\n      <th>AdjustedOpen</th>\n      <th>CumulativeAdjustmentFactorHigh</th>\n      <th>AdjustedHigh</th>\n      <th>CumulativeAdjustmentFactorLow</th>\n      <th>AdjustedLow</th>\n      <th>CumulativeAdjustmentFactorVolume</th>\n      <th>AdjustedVolume</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20170104_1301</td>\n      <td>2017-01-04</td>\n      <td>1301</td>\n      <td>2734.0</td>\n      <td>2755.0</td>\n      <td>2730.0</td>\n      <td>2742.0</td>\n      <td>31400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2742.0</td>\n      <td>1.0</td>\n      <td>2734.0</td>\n      <td>1.0</td>\n      <td>2755.0</td>\n      <td>1.0</td>\n      <td>2730.0</td>\n      <td>1.0</td>\n      <td>31400.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20170105_1301</td>\n      <td>2017-01-05</td>\n      <td>1301</td>\n      <td>2743.0</td>\n      <td>2747.0</td>\n      <td>2735.0</td>\n      <td>2738.0</td>\n      <td>17900</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2738.0</td>\n      <td>1.0</td>\n      <td>2743.0</td>\n      <td>1.0</td>\n      <td>2747.0</td>\n      <td>1.0</td>\n      <td>2735.0</td>\n      <td>1.0</td>\n      <td>17900.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20170106_1301</td>\n      <td>2017-01-06</td>\n      <td>1301</td>\n      <td>2734.0</td>\n      <td>2744.0</td>\n      <td>2720.0</td>\n      <td>2740.0</td>\n      <td>19900</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2740.0</td>\n      <td>1.0</td>\n      <td>2734.0</td>\n      <td>1.0</td>\n      <td>2744.0</td>\n      <td>1.0</td>\n      <td>2720.0</td>\n      <td>1.0</td>\n      <td>19900.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20170110_1301</td>\n      <td>2017-01-10</td>\n      <td>1301</td>\n      <td>2745.0</td>\n      <td>2754.0</td>\n      <td>2735.0</td>\n      <td>2748.0</td>\n      <td>24200</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2748.0</td>\n      <td>1.0</td>\n      <td>2745.0</td>\n      <td>1.0</td>\n      <td>2754.0</td>\n      <td>1.0</td>\n      <td>2735.0</td>\n      <td>1.0</td>\n      <td>24200.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20170111_1301</td>\n      <td>2017-01-11</td>\n      <td>1301</td>\n      <td>2748.0</td>\n      <td>2752.0</td>\n      <td>2737.0</td>\n      <td>2745.0</td>\n      <td>9300</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2745.0</td>\n      <td>1.0</td>\n      <td>2748.0</td>\n      <td>1.0</td>\n      <td>2752.0</td>\n      <td>1.0</td>\n      <td>2737.0</td>\n      <td>1.0</td>\n      <td>9300.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2436634</th>\n      <td>20220221_9997</td>\n      <td>2022-02-21</td>\n      <td>9997</td>\n      <td>725.0</td>\n      <td>729.0</td>\n      <td>719.0</td>\n      <td>727.0</td>\n      <td>116400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>727.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>729.0</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>116400.0</td>\n    </tr>\n    <tr>\n      <th>2436635</th>\n      <td>20220222_9997</td>\n      <td>2022-02-22</td>\n      <td>9997</td>\n      <td>719.0</td>\n      <td>723.0</td>\n      <td>711.0</td>\n      <td>721.0</td>\n      <td>225500</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>721.0</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>723.0</td>\n      <td>1.0</td>\n      <td>711.0</td>\n      <td>1.0</td>\n      <td>225500.0</td>\n    </tr>\n    <tr>\n      <th>2436636</th>\n      <td>20220224_9997</td>\n      <td>2022-02-24</td>\n      <td>9997</td>\n      <td>709.0</td>\n      <td>725.0</td>\n      <td>708.0</td>\n      <td>719.0</td>\n      <td>195600</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>719.0</td>\n      <td>1.0</td>\n      <td>709.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>708.0</td>\n      <td>1.0</td>\n      <td>195600.0</td>\n    </tr>\n    <tr>\n      <th>2436637</th>\n      <td>20220225_9997</td>\n      <td>2022-02-25</td>\n      <td>9997</td>\n      <td>725.0</td>\n      <td>738.0</td>\n      <td>724.0</td>\n      <td>733.0</td>\n      <td>170500</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>733.0</td>\n      <td>1.0</td>\n      <td>725.0</td>\n      <td>1.0</td>\n      <td>738.0</td>\n      <td>1.0</td>\n      <td>724.0</td>\n      <td>1.0</td>\n      <td>170500.0</td>\n    </tr>\n    <tr>\n      <th>2436638</th>\n      <td>20220228_9997</td>\n      <td>2022-02-28</td>\n      <td>9997</td>\n      <td>731.0</td>\n      <td>737.0</td>\n      <td>726.0</td>\n      <td>734.0</td>\n      <td>288100</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>734.0</td>\n      <td>1.0</td>\n      <td>731.0</td>\n      <td>1.0</td>\n      <td>737.0</td>\n      <td>1.0</td>\n      <td>726.0</td>\n      <td>1.0</td>\n      <td>288100.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2436639 rows × 37 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Featrue","metadata":{}},{"cell_type":"code","source":"def cal_moving_average(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"MovingAverage{key}{period}\"\n            col_gap = f\"{col}GapPercent\"\n            df[col] = df[key].rolling(period, min_periods=1).mean()\n            df[col_gap] = (df[key] / df[col]) * 100.0\n        return df\n    return func\n\ndef cal_changing_ration(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"ChangingRatio{key}{period}\"\n            df[col] = df[key].pct_change(period) * 100\n        return df\n    return func\n\ndef cal_historical_vix(key: str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"HistoricalVIX{key}{period}\"\n            df[col] = np.log(df[key]).diff().rolling(period).std()\n        return df\n    return func\n\ndef add_columns_per_code(df, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    df = df.sort_values([\"SecuritiesCode\", \"Date\"])\n    df = df.groupby(\"SecuritiesCode\").apply(func)\n    df = df.reset_index(drop=True)\n    return df\n\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\ndef generate_features(df):\n    base_df = df.copy()\n    prev_column_names = base_df.columns\n    periods = [5, 25, 75]\n    functions = [\n        cal_moving_average(\"AdjustedClose\", periods),\n        cal_moving_average(\"AdjustedOpen\", periods),\n        cal_moving_average(\"AdjustedHigh\", periods),\n        cal_moving_average(\"AdjustedLow\", periods),\n        cal_moving_average(\"AdjustedVolume\", periods),\n        cal_changing_ration(\"AdjustedClose\", periods),\n        cal_changing_ration(\"AdjustedOpen\", periods),\n        cal_changing_ration(\"AdjustedHigh\", periods),\n        cal_changing_ration(\"AdjustedLow\", periods),\n        cal_changing_ration(\"AdjustedVolume\", periods),\n        cal_historical_vix(\"AdjustedClose\", periods),\n        cal_historical_vix(\"AdjustedOpen\", periods),\n        cal_historical_vix(\"AdjustedHigh\", periods),\n        cal_historical_vix(\"AdjustedLow\", periods),\n        cal_historical_vix(\"AdjustedVolume\", periods)\n    ]\n    \n    base_df = add_columns_per_code(base_df, functions)\n    base_df = add_columns_per_day(base_df)\n    \n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    #feats = feats[feats[\"HistoricalVIXAdjustedClose75\"] != 0]\n    return base_df, add_column_names\n\ndef select_features(feature_df, add_column_names, is_train):\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    numerical_cols = sorted(add_column_names)\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n    label_col = ['Target']\n    feat_cols = numerical_cols + categorical_cols\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    if is_train:\n        feature_df.dropna(inplace=True)\n    else:\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n    return feature_df, feat_cols, label_col\n\ndef preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n\n    return feature_df, feat_cols, label_col\n\nfeature_df, feat_cols, label_col = preprocessor(train_df)\n\n# modelの結果をもとにfeat_colsを上書き\nfeat_cols = ['33SectorCode', 'ChangingRatioAdjustedVolume25', 'diff_rate2', 'MovingAverageAdjustedHigh5GapPercent', 'MovingAverageAdjustedOpen5GapPercent', 'HistoricalVIXAdjustedLow5', 'MovingAverageAdjustedClose5GapPercent', 'HistoricalVIXAdjustedOpen5', 'MovingAverageAdjustedLow25GapPercent', 'ChangingRatioAdjustedVolume5', 'HistoricalVIXAdjustedOpen75', 'HistoricalVIXAdjustedVolume5', 'MovingAverageAdjustedVolume25GapPercent', 'MovingAverageAdjustedVolume5', 'diff_rate1', 'ChangingRatioAdjustedHigh5', 'ChangingRatioAdjustedOpen25', 'HistoricalVIXAdjustedOpen25', 'MovingAverageAdjustedClose25GapPercent', 'MovingAverageAdjustedVolume75GapPercent', 'ChangingRatioAdjustedLow25', 'ChangingRatioAdjustedLow5', 'HistoricalVIXAdjustedHigh75', 'MovingAverageAdjustedLow5GapPercent', 'ChangingRatioAdjustedClose75', 'MovingAverageAdjustedClose5', 'MovingAverageAdjustedClose75', 'MovingAverageAdjustedClose75GapPercent', 'HistoricalVIXAdjustedVolume75']\nfeat_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-21T15:26:13.397057Z","iopub.execute_input":"2022-04-21T15:26:13.397394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio\n\n# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score\n\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n# 学習を実行する関数\ndef trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    scores = []\n    models = []\n    params = []\n    i = 0\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n        \n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        ################################\n        # 学習\n        ################################\n        params = {\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'regression',         # 回帰\n            'metric': 'rmse',                  # 損失（誤差）\n            'learning_rate': 0.01,             # 学習率\n            'lambda_l1': 0.5,                  # L1正則化項の係数\n            'lambda_l2': 0.5,                  # L2正則化項の係数\n            'num_leaves': 10,                  # 最大葉枚数\n            'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n            'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n            'bagging_freq': 5,                 # バギング実施頻度\n            'min_child_samples': 10,           # 葉に含まれる最小データ数\n            'seed': seed                       # シード値\n        } \n \n        lgb_results = {}                       \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=2000,              # 計算回数\n            early_stopping_rounds=100,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 4))\n\n        # loss\n        plt.subplot(1,2,1)\n        loss_train = lgb_results['Train']['rmse']\n        loss_test = lgb_results['Valid']['rmse']   \n        plt.xlabel('Iteration')\n        plt.ylabel('logloss')\n        plt.plot(loss_train, label='train loss')\n        plt.plot(loss_test, label='valid loss')\n        plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        write_df(importance, f\"importance_{i}\")\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n\n        ################################\n        # 評価\n        ################################\n        # 推論\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        # 評価\n        score = evaluator(valid, pred)\n\n        scores.append(score)\n        models.append(model)\n        # save model\n        model.save_model(f'model_{i}.txt')\n        i = i + 1\n        # model = lightgbm.Booster(model_file='lgbr_base.txt')\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\nmodels = trainer(feature_df, feat_cols, label_col, fold_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}