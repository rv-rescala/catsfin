{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T22:33:21.560092Z","iopub.execute_input":"2022-04-21T22:33:21.560556Z","iopub.status.idle":"2022-04-21T22:33:22.886822Z","shell.execute_reply.started":"2022-04-21T22:33:21.560426Z","shell.execute_reply":"2022-04-21T22:33:22.885972Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# I/O Func\nBASE_PATH = Path(f'/kaggle/working')\n\ndef adjusting_price(price, key: str):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[f\"CumulativeAdjustmentFactor{key}\"] * df[key]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef adjusting_volume(price, key = \"Volume\"):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[key] / df[f\"CumulativeAdjustmentFactor{key}\"]  \n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef read_prices(dir_name: str, securities_code: int = None):\n    \"\"\"[Important: the dateset of 2020/10/1 is lost because of system failer in JPX, see: https://www.jpx.co.jp/corporate/news/news-releases/0060/20201019-01.html]\n    \n    \"\"\"\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    df = pd.read_csv(base_path / 'stock_prices.csv')\n    df.loc[: ,\"Date\"] = pd.to_datetime(df.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n    df = df[df['Open'].notna()]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_stock_list(securities_code: int = None, only_universe: bool = True):\n    df = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\n    df.loc[: ,\"EffectiveDate\"] = pd.to_datetime(df.loc[: ,\"EffectiveDate\"], format=\"%Y%m%d\")\n    if only_universe:\n        df = df[df['Universe0']]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_train_data_by_price(securities_code: int = None, with_supplemental: bool = True):\n    \"\"\"[The train base is price dataset, the other data are joined to prices DF by left join]\n    \n    \"\"\"\n    def merge_data(prices, stock_list):\n        base_df = prices.copy()\n        _stock_list = stock_list.copy()\n        _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n        base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n        return base_df\n    \n    # origin\n    df = merge_data(prices=read_prices(dir_name=\"train_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n    \n    # supplyment\n    if with_supplemental:\n        supplemental_df = merge_data(prices=read_prices(dir_name=\"supplemental_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n        df = pd.concat([df, supplemental_df]).reset_index(drop=True)\n        \n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef write_df(df, filename):\n    df.to_csv(BASE_PATH / f'{filename}.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:33:22.888402Z","iopub.execute_input":"2022-04-21T22:33:22.888683Z","iopub.status.idle":"2022-04-21T22:33:22.918952Z","shell.execute_reply.started":"2022-04-21T22:33:22.888653Z","shell.execute_reply":"2022-04-21T22:33:22.917740Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = read_train_data_by_price()\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:33:22.920273Z","iopub.execute_input":"2022-04-21T22:33:22.920854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featrue","metadata":{}},{"cell_type":"code","source":"def cal_moving_average(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"MovingAverage{key}{period}\"\n            col_gap = f\"{col}GapPercent\"\n            df[col] = df[key].rolling(period, min_periods=1).mean()\n            df[col_gap] = (df[key] / df[col]) * 100.0\n        return df\n    return func\n\ndef cal_changing_ration(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"ChangingRatio{key}{period}\"\n            df[col] = df[key].pct_change(period) * 100\n        return df\n    return func\n\ndef cal_historical_vix(key: str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"HistoricalVIX{key}{period}\"\n            df[col] = np.log(df[key]).diff().rolling(period).std()\n        return df\n    return func\n\ndef add_columns_per_code(df, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    df = df.sort_values([\"SecuritiesCode\", \"Date\"])\n    df = df.groupby(\"SecuritiesCode\").apply(func)\n    df = df.reset_index(drop=True)\n    return df\n\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\ndef generate_features(df):\n    base_df = df.copy()\n    prev_column_names = base_df.columns\n    periods = [5, 25, 75]\n    functions = [\n        cal_moving_average(\"AdjustedClose\", periods),\n        cal_moving_average(\"AdjustedOpen\", periods),\n        cal_moving_average(\"AdjustedHigh\", periods),\n        cal_moving_average(\"AdjustedLow\", periods),\n        cal_moving_average(\"AdjustedVolume\", periods),\n        cal_changing_ration(\"AdjustedClose\", periods),\n        cal_changing_ration(\"AdjustedOpen\", periods),\n        cal_changing_ration(\"AdjustedHigh\", periods),\n        cal_changing_ration(\"AdjustedLow\", periods),\n        cal_changing_ration(\"AdjustedVolume\", periods),\n        cal_historical_vix(\"AdjustedClose\", periods),\n        cal_historical_vix(\"AdjustedOpen\", periods),\n        cal_historical_vix(\"AdjustedHigh\", periods),\n        cal_historical_vix(\"AdjustedLow\", periods),\n        cal_historical_vix(\"AdjustedVolume\", periods)\n    ]\n    \n    base_df = add_columns_per_code(base_df, functions)\n    base_df = add_columns_per_day(base_df)\n    \n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    #feats = feats[feats[\"HistoricalVIXAdjustedClose75\"] != 0]\n    return base_df, add_column_names\n\ndef select_features(feature_df, add_column_names, is_train):\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    numerical_cols = sorted(add_column_names)\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n    label_col = ['Target']\n    feat_cols = numerical_cols + categorical_cols\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    if is_train:\n        feature_df.dropna(inplace=True)\n    else:\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n    return feature_df, feat_cols, label_col\n\ndef preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n\n    return feature_df, feat_cols, label_col\n\nfeature_df, feat_cols, label_col = preprocessor(train_df)\n\n# modelの結果をもとにfeat_colsを上書き\nfeat_cols = ['33SectorCode', 'ChangingRatioAdjustedVolume25', 'diff_rate2', 'MovingAverageAdjustedHigh5GapPercent', 'MovingAverageAdjustedOpen5GapPercent', 'HistoricalVIXAdjustedLow5', 'MovingAverageAdjustedClose5GapPercent', 'HistoricalVIXAdjustedOpen5', 'MovingAverageAdjustedLow25GapPercent', 'ChangingRatioAdjustedVolume5', 'HistoricalVIXAdjustedOpen75', 'HistoricalVIXAdjustedVolume5', 'MovingAverageAdjustedVolume25GapPercent', 'MovingAverageAdjustedVolume5', 'diff_rate1', 'ChangingRatioAdjustedHigh5', 'ChangingRatioAdjustedOpen25', 'HistoricalVIXAdjustedOpen25', 'MovingAverageAdjustedClose25GapPercent', 'MovingAverageAdjustedVolume75GapPercent', 'ChangingRatioAdjustedLow25', 'ChangingRatioAdjustedLow5', 'HistoricalVIXAdjustedHigh75', 'MovingAverageAdjustedLow5GapPercent', 'ChangingRatioAdjustedClose75', 'MovingAverageAdjustedClose5', 'MovingAverageAdjustedClose75', 'MovingAverageAdjustedClose75GapPercent', 'HistoricalVIXAdjustedVolume75']\nfeat_cols","metadata":{"execution":{"iopub.status.idle":"2022-04-21T22:37:39.929151Z","shell.execute_reply.started":"2022-04-21T22:35:36.284690Z","shell.execute_reply":"2022-04-21T22:37:39.928348Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['33SectorCode',\n 'ChangingRatioAdjustedVolume25',\n 'diff_rate2',\n 'MovingAverageAdjustedHigh5GapPercent',\n 'MovingAverageAdjustedOpen5GapPercent',\n 'HistoricalVIXAdjustedLow5',\n 'MovingAverageAdjustedClose5GapPercent',\n 'HistoricalVIXAdjustedOpen5',\n 'MovingAverageAdjustedLow25GapPercent',\n 'ChangingRatioAdjustedVolume5',\n 'HistoricalVIXAdjustedOpen75',\n 'HistoricalVIXAdjustedVolume5',\n 'MovingAverageAdjustedVolume25GapPercent',\n 'MovingAverageAdjustedVolume5',\n 'diff_rate1',\n 'ChangingRatioAdjustedHigh5',\n 'ChangingRatioAdjustedOpen25',\n 'HistoricalVIXAdjustedOpen25',\n 'MovingAverageAdjustedClose25GapPercent',\n 'MovingAverageAdjustedVolume75GapPercent',\n 'ChangingRatioAdjustedLow25',\n 'ChangingRatioAdjustedLow5',\n 'HistoricalVIXAdjustedHigh75',\n 'MovingAverageAdjustedLow5GapPercent',\n 'ChangingRatioAdjustedClose75',\n 'MovingAverageAdjustedClose5',\n 'MovingAverageAdjustedClose75',\n 'MovingAverageAdjustedClose75GapPercent',\n 'HistoricalVIXAdjustedVolume75']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Learning","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    return sharpe_ratio\n\n# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score\n\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n# 学習を実行する関数\ndef trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):\n    scores = []\n    models = []\n    params = []\n    i = 0\n    for param in fold_params:\n        ################################\n        # データ準備\n        ################################\n        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n        X_train = train[feat_cols]\n        y_train = train[label_col]\n        X_valid = valid[feat_cols]\n        y_valid = valid[label_col]\n        \n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        ################################\n        # 学習\n        ################################\n        params = {\n            'task': 'train',                   # 学習\n            'boosting_type': 'gbdt',           # GBDT\n            'objective': 'regression',         # 回帰\n            'metric': 'rmse',                  # 損失（誤差）\n            'learning_rate': 0.01,             # 学習率\n            'lambda_l1': 0.5,                  # L1正則化項の係数\n            'lambda_l2': 0.5,                  # L2正則化項の係数\n            'num_leaves': 10,                  # 最大葉枚数\n            'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n            'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n            'bagging_freq': 5,                 # バギング実施頻度\n            'min_child_samples': 10,           # 葉に含まれる最小データ数\n            'seed': seed                       # シード値\n        } \n \n        lgb_results = {}                       \n        model = lgb.train( \n            params,                            # ハイパーパラメータ\n            lgb_train,                         # 訓練データ\n            valid_sets=[lgb_train, lgb_valid], # 検証データ\n            valid_names=['Train', 'Valid'],    # データセット名前\n            num_boost_round=2000,              # 計算回数\n            early_stopping_rounds=100,         # 計算打ち切り設定\n            evals_result=lgb_results,          # 学習の履歴\n            verbose_eval=100,                  # 学習過程の表示サイクル\n        )  \n\n        ################################\n        # 結果描画\n        ################################\n        fig = plt.figure(figsize=(10, 4))\n\n        # loss\n        plt.subplot(1,2,1)\n        loss_train = lgb_results['Train']['rmse']\n        loss_test = lgb_results['Valid']['rmse']   \n        plt.xlabel('Iteration')\n        plt.ylabel('logloss')\n        plt.plot(loss_train, label='train loss')\n        plt.plot(loss_test, label='valid loss')\n        plt.legend()\n\n        # feature importance\n        plt.subplot(1,2,2)\n        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n        write_df(importance, f\"importance_{i}\")\n        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n        plt.tight_layout()\n        plt.show()\n\n        ################################\n        # 評価\n        ################################\n        # 推論\n        pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n        # 評価\n        score = evaluator(valid, pred)\n\n        scores.append(score)\n        models.append(model)\n        # save model\n        model.save_model(f'{BASE_PATH} / model_{i}.txt')\n        i = i + 1\n        # model = lightgbm.Booster(model_file='lgbr_base.txt')\n\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:37:39.930784Z","iopub.execute_input":"2022-04-21T22:37:39.931285Z","iopub.status.idle":"2022-04-21T22:37:41.579061Z","shell.execute_reply.started":"2022-04-21T22:37:39.931249Z","shell.execute_reply":"2022-04-21T22:37:41.578068Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\nmodels = trainer(feature_df, feat_cols, label_col, fold_params)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T22:37:41.580242Z","iopub.execute_input":"2022-04-21T22:37:41.580476Z","iopub.status.idle":"2022-04-21T22:40:12.345866Z","shell.execute_reply.started":"2022-04-21T22:37:41.580446Z","shell.execute_reply":"2022-04-21T22:40:12.344359Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-04-21 22:37:41,754]\u001b[0m A new study created in memory with name: no-name-4b4a3694-c921-4962-af7a-b5a864aab57a\u001b[0m\nfeature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  14%|#4        | 1/7 [00:04<00:28,  4.69s/it]\u001b[32m[I 2022-04-21 22:37:46,454]\u001b[0m Trial 0 finished with value: 0.02464463239883677 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  14%|#4        | 1/7 [00:04<00:28,  4.69s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214215\tValid's rmse: 0.0246646\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214585\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  29%|##8       | 2/7 [00:08<00:21,  4.22s/it]\u001b[32m[I 2022-04-21 22:37:50,343]\u001b[0m Trial 1 finished with value: 0.024644802843216466 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  29%|##8       | 2/7 [00:08<00:21,  4.22s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214213\tValid's rmse: 0.0246646\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214585\tValid's rmse: 0.0246448\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  43%|####2     | 3/7 [00:12<00:15,  3.94s/it]\u001b[32m[I 2022-04-21 22:37:53,957]\u001b[0m Trial 2 finished with value: 0.02464463239883677 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  43%|####2     | 3/7 [00:12<00:15,  3.94s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.021422\tValid's rmse: 0.0246653\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214585\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  57%|#####7    | 4/7 [00:15<00:11,  3.78s/it]\u001b[32m[I 2022-04-21 22:37:57,493]\u001b[0m Trial 3 finished with value: 0.02464486665625294 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  57%|#####7    | 4/7 [00:15<00:11,  3.78s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214247\tValid's rmse: 0.0246661\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214586\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  71%|#######1  | 5/7 [00:19<00:07,  3.70s/it]\u001b[32m[I 2022-04-21 22:38:01,058]\u001b[0m Trial 4 finished with value: 0.024644682109811286 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  71%|#######1  | 5/7 [00:19<00:07,  3.70s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214234\tValid's rmse: 0.0246646\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214586\tValid's rmse: 0.0246447\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645:  86%|########5 | 6/7 [00:23<00:03,  3.81s/it]\u001b[32m[I 2022-04-21 22:38:05,086]\u001b[0m Trial 5 finished with value: 0.024644677040188058 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645:  86%|########5 | 6/7 [00:23<00:03,  3.81s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.021421\tValid's rmse: 0.024666\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214585\tValid's rmse: 0.0246447\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"feature_fraction, val_score: 0.024645: 100%|##########| 7/7 [00:26<00:00,  3.69s/it]\u001b[32m[I 2022-04-21 22:38:08,535]\u001b[0m Trial 6 finished with value: 0.024644820662975717 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.02464463239883677.\u001b[0m\nfeature_fraction, val_score: 0.024645: 100%|##########| 7/7 [00:26<00:00,  3.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214265\tValid's rmse: 0.0246637\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214586\tValid's rmse: 0.0246448\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:   5%|5         | 1/20 [00:04<01:19,  4.20s/it]\u001b[32m[I 2022-04-21 22:38:12,737]\u001b[0m Trial 7 finished with value: 0.02464464975344699 and parameters: {'num_leaves': 16}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:   5%|5         | 1/20 [00:04<01:19,  4.20s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.021409\tValid's rmse: 0.0246651\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214584\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075534 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  10%|#         | 2/20 [00:10<01:35,  5.30s/it]\u001b[32m[I 2022-04-21 22:38:18,805]\u001b[0m Trial 8 finished with value: 0.024644705773761863 and parameters: {'num_leaves': 31}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  10%|#         | 2/20 [00:10<01:35,  5.30s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213859\tValid's rmse: 0.024666\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214582\tValid's rmse: 0.0246447\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  15%|#5        | 3/20 [00:19<01:56,  6.88s/it]\u001b[32m[I 2022-04-21 22:38:27,567]\u001b[0m Trial 9 finished with value: 0.024644957193690985 and parameters: {'num_leaves': 186}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  15%|#5        | 3/20 [00:19<01:56,  6.88s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.021268\tValid's rmse: 0.024674\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214568\tValid's rmse: 0.024645\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073681 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  20%|##        | 4/20 [00:27<02:02,  7.67s/it]\u001b[32m[I 2022-04-21 22:38:36,439]\u001b[0m Trial 10 finished with value: 0.024645013329043588 and parameters: {'num_leaves': 204}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  20%|##        | 4/20 [00:27<02:02,  7.67s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0212584\tValid's rmse: 0.0246728\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214567\tValid's rmse: 0.024645\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092887 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  25%|##5       | 5/20 [00:33<01:45,  7.02s/it]\u001b[32m[I 2022-04-21 22:38:42,322]\u001b[0m Trial 11 finished with value: 0.024644786031698444 and parameters: {'num_leaves': 47}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  25%|##5       | 5/20 [00:33<01:45,  7.02s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213673\tValid's rmse: 0.0246688\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.021458\tValid's rmse: 0.0246448\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070932 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  30%|###       | 6/20 [00:42<01:46,  7.62s/it]\u001b[32m[I 2022-04-21 22:38:51,105]\u001b[0m Trial 12 finished with value: 0.024644946580947322 and parameters: {'num_leaves': 121}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  30%|###       | 6/20 [00:42<01:46,  7.62s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213069\tValid's rmse: 0.0246735\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214572\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  35%|###5      | 7/20 [00:51<01:44,  8.05s/it]\u001b[32m[I 2022-04-21 22:39:00,051]\u001b[0m Trial 13 finished with value: 0.024644991860950625 and parameters: {'num_leaves': 170}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  35%|###5      | 7/20 [00:51<01:44,  8.05s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0212774\tValid's rmse: 0.0246749\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214569\tValid's rmse: 0.024645\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  40%|####      | 8/20 [00:59<01:37,  8.16s/it]\u001b[32m[I 2022-04-21 22:39:08,449]\u001b[0m Trial 14 finished with value: 0.024644943960414577 and parameters: {'num_leaves': 148}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  40%|####      | 8/20 [00:59<01:37,  8.16s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0212901\tValid's rmse: 0.0246741\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.021457\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  45%|####5     | 9/20 [01:07<01:28,  8.08s/it]\u001b[32m[I 2022-04-21 22:39:16,332]\u001b[0m Trial 15 finished with value: 0.02464494891213401 and parameters: {'num_leaves': 138}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  45%|####5     | 9/20 [01:07<01:28,  8.08s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0212966\tValid's rmse: 0.0246745\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214571\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  50%|#####     | 10/20 [01:15<01:18,  7.86s/it]\u001b[32m[I 2022-04-21 22:39:23,703]\u001b[0m Trial 16 finished with value: 0.024644938537409913 and parameters: {'num_leaves': 68}. Best is trial 7 with value: 0.02464464975344699.\u001b[0m\nnum_leaves, val_score: 0.024645:  50%|#####     | 10/20 [01:15<01:18,  7.86s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213471\tValid's rmse: 0.0246707\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214577\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  55%|#####5    | 11/20 [01:18<00:58,  6.48s/it]\u001b[32m[I 2022-04-21 22:39:27,046]\u001b[0m Trial 17 finished with value: 0.024644641398044835 and parameters: {'num_leaves': 6}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  55%|#####5    | 11/20 [01:18<00:58,  6.48s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214322\tValid's rmse: 0.0246658\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214587\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076766 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  60%|######    | 12/20 [01:21<00:44,  5.55s/it]\u001b[32m[I 2022-04-21 22:39:30,471]\u001b[0m Trial 18 finished with value: 0.02464464516211728 and parameters: {'num_leaves': 7}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  60%|######    | 12/20 [01:21<00:44,  5.55s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214291\tValid's rmse: 0.0246648\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214586\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069316 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  65%|######5   | 13/20 [01:31<00:47,  6.84s/it]\u001b[32m[I 2022-04-21 22:39:40,296]\u001b[0m Trial 19 finished with value: 0.02464507101412355 and parameters: {'num_leaves': 253}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  65%|######5   | 13/20 [01:31<00:47,  6.84s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.021235\tValid's rmse: 0.0246737\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214565\tValid's rmse: 0.0246451\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  70%|#######   | 14/20 [01:38<00:41,  6.87s/it]\u001b[32m[I 2022-04-21 22:39:47,220]\u001b[0m Trial 20 finished with value: 0.02464483272949999 and parameters: {'num_leaves': 87}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  70%|#######   | 14/20 [01:38<00:41,  6.87s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213315\tValid's rmse: 0.0246726\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214575\tValid's rmse: 0.0246448\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070583 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  75%|#######5  | 15/20 [01:42<00:29,  5.80s/it]\u001b[32m[I 2022-04-21 22:39:50,545]\u001b[0m Trial 21 finished with value: 0.024644641398044835 and parameters: {'num_leaves': 6}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  75%|#######5  | 15/20 [01:42<00:29,  5.80s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214322\tValid's rmse: 0.0246658\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214587\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  80%|########  | 16/20 [01:49<00:25,  6.46s/it]\u001b[32m[I 2022-04-21 22:39:58,535]\u001b[0m Trial 22 finished with value: 0.024644868569703618 and parameters: {'num_leaves': 92}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  80%|########  | 16/20 [01:49<00:25,  6.46s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213276\tValid's rmse: 0.0246736\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214574\tValid's rmse: 0.0246449\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  85%|########5 | 17/20 [01:55<00:18,  6.31s/it]\u001b[32m[I 2022-04-21 22:40:04,498]\u001b[0m Trial 23 finished with value: 0.02464484465134234 and parameters: {'num_leaves': 54}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  85%|########5 | 17/20 [01:55<00:18,  6.31s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0213602\tValid's rmse: 0.0246698\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214579\tValid's rmse: 0.0246448\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"name":"stderr","text":"num_leaves, val_score: 0.024645:  90%|######### | 18/20 [01:59<00:10,  5.49s/it]\u001b[32m[I 2022-04-21 22:40:08,075]\u001b[0m Trial 24 finished with value: 0.024644641398044835 and parameters: {'num_leaves': 6}. Best is trial 17 with value: 0.024644641398044835.\u001b[0m\nnum_leaves, val_score: 0.024645:  90%|######### | 18/20 [01:59<00:10,  5.49s/it]","output_type":"stream"},{"name":"stdout","text":"[100]\tTrain's rmse: 0.0214322\tValid's rmse: 0.0246658\nEarly stopping, best iteration is:\n[1]\tTrain's rmse: 0.0214587\tValid's rmse: 0.0246446\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102419 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7173\n[LightGBM] [Info] Number of data points in the train set: 416216, number of used features: 29\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n[LightGBM] [Info] Start training from score 0.000742\nTraining until validation scores don't improve for 100 rounds\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/3574298542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'2021-02-23'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2022-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2022-02-01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_33/962554045.py\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(feature_df, feat_cols, label_col, fold_params, seed)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# 計算打ち切り設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_results\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# 学習の履歴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# 学習過程の表示サイクル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         )  \n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/integration/_lightgbm_tuner/__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mauto_booster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightGBMTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mauto_booster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_booster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_feature_fraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_num_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_bagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_feature_fraction_stage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\u001b[0m in \u001b[0;36mtune_num_leaves\u001b[0;34m(self, n_trials)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optuna_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0;34m\"num_leaves\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\u001b[0m in \u001b[0;36m_tune_params\u001b[0;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                 \u001b[0mcatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optuna_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m             )\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgbm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid_sets\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_valid_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlgbm_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_booster_best_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3021\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}