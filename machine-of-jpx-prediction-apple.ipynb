{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom decimal import ROUND_HALF_UP, Decimal\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T07:02:44.797184Z","iopub.execute_input":"2022-04-22T07:02:44.797937Z","iopub.status.idle":"2022-04-22T07:02:46.182824Z","shell.execute_reply.started":"2022-04-22T07:02:44.797845Z","shell.execute_reply":"2022-04-22T07:02:46.181804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I/O Func\nMODEL_NAME = \"APPLE\"\nBASE_PATH = Path(f'/kaggle/working')\n\ndef adjusting_price(price, key: str):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[f\"CumulativeAdjustmentFactor{key}\"] * df[key]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef adjusting_volume(price, key = \"Volume\"):\n    \"\"\"[Adjusting Close Price]\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n\n    def generate_adjusted(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, f\"CumulativeAdjustmentFactor{key}\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = (\n            df[key] / df[f\"CumulativeAdjustmentFactor{key}\"]  \n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[f\"Adjusted{key}\"] == 0, f\"Adjusted{key}\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, f\"Adjusted{key}\"] = df.loc[:, f\"Adjusted{key}\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted).reset_index(drop=True)\n\n    # price.set_index(\"Date\", inplace=True)\n    return price\n\ndef read_prices(dir_name: str, securities_code: int = None):\n    \"\"\"[Important: the dateset of 2020/10/1 is lost because of system failer in JPX, see: https://www.jpx.co.jp/corporate/news/news-releases/0060/20201019-01.html]\n    \n    \"\"\"\n    base_path = Path(f'../input/jpx-tokyo-stock-exchange-prediction/{dir_name}')\n    df = pd.read_csv(base_path / 'stock_prices.csv')\n    df.loc[: ,\"Date\"] = pd.to_datetime(df.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n    df = df[df['Open'].notna()]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef read_stock_list(securities_code: int = None, only_universe: bool = True):\n    df = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv')\n    df.loc[: ,\"EffectiveDate\"] = pd.to_datetime(df.loc[: ,\"EffectiveDate\"], format=\"%Y%m%d\")\n    if only_universe:\n        df = df[df['Universe0']]\n    if securities_code:\n        df = df[df[\"SecuritiesCode\"] == securities_code]\n    return df\n\ndef merge_data(prices, stock_list):\n    # stock_prices がベース\n    base_df = prices.copy()\n    \n    # stock_listと結合\n    _stock_list = stock_list.copy()\n    _stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)\n    base_df = base_df.merge(_stock_list, on='SecuritiesCode', how=\"left\")\n    \n    return base_df\n\ndef read_train_data_by_price(securities_code: int = None, with_supplemental: bool = True):\n    \"\"\"[The train base is price dataset, the other data are joined to prices DF by left join]\n    \n    \"\"\"\n    # origin\n    df = merge_data(prices=read_prices(dir_name=\"train_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n    \n    # supplyment\n    if with_supplemental:\n        supplemental_df = merge_data(prices=read_prices(dir_name=\"supplemental_files\", securities_code=securities_code), stock_list=read_stock_list(securities_code=securities_code))\n        df = pd.concat([df, supplemental_df]).reset_index(drop=True)\n        \n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef collector(prices, options, financials, trades, secondary_prices, stock_list):\n    # 読み込んだデータを統合して一つのファイルに纏める\n    df = merge_data(prices, stock_list)\n    # AdjustedClose項目の生成\n    df = adjusting_price(df, \"Close\")\n    df = adjusting_price(df, \"Open\")\n    df = adjusting_price(df, \"High\")\n    df = adjusting_price(df, \"Low\")\n    df = adjusting_volume(df)\n    return df\n\ndef write_df(df, filename):\n    df.to_csv(f'{BASE_PATH}/{filename}_{MODEL_NAME}.csv',index = False)\n    \nimport joblib\ndef write_model(model, name):\n    # save model\n    joblib.dump(model, f'{BASE_PATH}/{name}_{MODEL_NAME}.pkl')\n\n\n# load model\ndef read_model(name):\n    return joblib.load(f'{BASE_PATH}/{name}_{MODEL_NAME}.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:02:46.184927Z","iopub.execute_input":"2022-04-22T07:02:46.187392Z","iopub.status.idle":"2022-04-22T07:02:46.219604Z","shell.execute_reply.started":"2022-04-22T07:02:46.187339Z","shell.execute_reply":"2022-04-22T07:02:46.218416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_list = read_stock_list()\nstock_list","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:02:46.220588Z","iopub.execute_input":"2022-04-22T07:02:46.221283Z","iopub.status.idle":"2022-04-22T07:02:46.323393Z","shell.execute_reply.started":"2022-04-22T07:02:46.221246Z","shell.execute_reply":"2022-04-22T07:02:46.32257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = read_train_data_by_price()\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:02:46.326092Z","iopub.execute_input":"2022-04-22T07:02:46.326474Z","iopub.status.idle":"2022-04-22T07:05:05.299819Z","shell.execute_reply.started":"2022-04-22T07:02:46.326431Z","shell.execute_reply":"2022-04-22T07:05:05.298994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featrue","metadata":{}},{"cell_type":"code","source":"def cal_moving_average(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"MovingAverage{key}{period}\"\n            col_gap = f\"{col}GapPercent\"\n            df[col] = df[key].rolling(period, min_periods=1).mean()\n            df[col_gap] = (df[key] / df[col]) * 100.0\n        return df\n    return func\n\ndef cal_changing_ration(key:str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"ChangingRatio{key}{period}\"\n            df[col] = df[key].pct_change(period) * 100\n        return df\n    return func\n\ndef cal_historical_vix(key: str, periods):\n    def func(df):\n        for period in periods:\n            col = f\"HistoricalVIX{key}{period}\"\n            df[col] = np.log(df[key]).diff().rolling(period).std()\n        return df\n    return func\n\ndef add_columns_per_code(df, functions):\n    def func(df):\n        for f in functions:\n            df = f(df)\n        return df\n    df = df.sort_values([\"SecuritiesCode\", \"Date\"])\n    df = df.groupby(\"SecuritiesCode\").apply(func)\n    df = df.reset_index(drop=True)\n    return df\n\ndef add_columns_per_day(base_df):\n    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']\n    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    \n    return base_df\n\ndef generate_features(df):\n    base_df = df.copy()\n    prev_column_names = base_df.columns\n    periods = [5, 25, 75]\n    functions = [\n        cal_moving_average(\"AdjustedClose\", periods),\n        cal_moving_average(\"AdjustedOpen\", periods),\n        cal_moving_average(\"AdjustedHigh\", periods),\n        cal_moving_average(\"AdjustedLow\", periods),\n        cal_moving_average(\"AdjustedVolume\", periods),\n        cal_changing_ration(\"AdjustedClose\", periods),\n        cal_changing_ration(\"AdjustedOpen\", periods),\n        cal_changing_ration(\"AdjustedHigh\", periods),\n        cal_changing_ration(\"AdjustedLow\", periods),\n        cal_changing_ration(\"AdjustedVolume\", periods),\n        cal_historical_vix(\"AdjustedClose\", periods),\n        cal_historical_vix(\"AdjustedOpen\", periods),\n        cal_historical_vix(\"AdjustedHigh\", periods),\n        cal_historical_vix(\"AdjustedLow\", periods),\n        cal_historical_vix(\"AdjustedVolume\", periods)\n    ]\n    \n    base_df = add_columns_per_code(base_df, functions)\n    base_df = add_columns_per_day(base_df)\n    \n    add_column_names = list(set(base_df.columns) - set(prev_column_names))\n    #feats = feats[feats[\"HistoricalVIXAdjustedClose75\"] != 0]\n    return base_df, add_column_names\n\ndef select_features(feature_df, add_column_names, is_train):\n    base_cols = ['RowId', 'Date', 'SecuritiesCode']\n    numerical_cols = sorted(add_column_names)\n    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']\n    label_col = ['Target']\n    feat_cols = numerical_cols + categorical_cols\n    feature_df = feature_df[base_cols + feat_cols + label_col]\n    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')\n    if is_train:\n        feature_df.dropna(inplace=True)\n    else:\n        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)\n        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)\n    return feature_df, feat_cols, label_col\n\ndef preprocessor(base_df, is_train=True):\n    feature_df = base_df.copy()\n    \n    ## 特徴量生成\n    feature_df, add_column_names = generate_features(feature_df)\n    \n    ## 特徴量選択\n    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)\n\n    # 上書き\n    feat_cols = ['33SectorCode', 'ChangingRatioAdjustedVolume25', 'diff_rate2', 'MovingAverageAdjustedHigh5GapPercent', 'MovingAverageAdjustedOpen5GapPercent', 'HistoricalVIXAdjustedLow5', 'MovingAverageAdjustedClose5GapPercent', 'HistoricalVIXAdjustedOpen5', 'MovingAverageAdjustedLow25GapPercent', 'ChangingRatioAdjustedVolume5', 'HistoricalVIXAdjustedOpen75', 'HistoricalVIXAdjustedVolume5', 'MovingAverageAdjustedVolume25GapPercent', 'diff_rate1', 'ChangingRatioAdjustedHigh5', 'ChangingRatioAdjustedOpen25', 'HistoricalVIXAdjustedOpen25', 'MovingAverageAdjustedClose25GapPercent', 'MovingAverageAdjustedVolume75GapPercent', 'ChangingRatioAdjustedLow25', 'ChangingRatioAdjustedLow5', 'HistoricalVIXAdjustedHigh75', 'MovingAverageAdjustedLow5GapPercent', 'ChangingRatioAdjustedClose75', 'MovingAverageAdjustedClose75', 'MovingAverageAdjustedClose75GapPercent', 'HistoricalVIXAdjustedVolume75']\n    return feature_df, feat_cols, label_col\n\nfeature_df, feat_cols, label_col = preprocessor(train_df)\nfeat_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:05:05.301361Z","iopub.execute_input":"2022-04-22T07:05:05.301616Z","iopub.status.idle":"2022-04-22T07:07:09.347903Z","shell.execute_reply.started":"2022-04-22T07:05:05.301581Z","shell.execute_reply":"2022-04-22T07:07:09.347054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning","metadata":{}},{"cell_type":"code","source":"# 予測値を降順に並べて順位番号を振る関数\n# 言い換えると、目的変数から提出用項目を導出する関数\ndef add_rank(df, col_name=\"pred\"):\n    df[\"Rank\"] = df.groupby(\"Date\")[col_name].rank(ascending=False, method=\"first\") - 1 \n    df[\"Rank\"] = df[\"Rank\"].astype(\"int\")\n    return df\n\ndef calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:\n    \"\"\"\n    Args:\n        df (pd.DataFrame): predicted results\n        portfolio_size (int): # of equities to buy/sell\n        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n    Returns:\n        (float): sharpe ratio\n    \"\"\"\n    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): predicted results\n            portfolio_size (int): # of equities to buy/sell\n            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.\n        Returns:\n            (float): spread return\n        \"\"\"\n        #print(f\"date: {df['Date'].min()}\")\n        #print(f\"min: {df['Rank'].min()}\")\n        #print(f\"max: {df['Rank'].max()}\")\n        assert df['Rank'].min() == 0\n        assert df['Rank'].max() == len(df['Rank']) - 1\n        weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n        purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()\n        #print(f\"short: {short}\")\n        return purchase - short\n\n    buf = df.groupby('Date').apply(_calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)\n    sharpe_ratio = buf.mean() / buf.std()\n    print(f\"sharpe_ratio: {sharpe_ratio}\")\n    return sharpe_ratio\n\n# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数\ndef evaluator(df, pred):\n    df[\"pred\"] = pred\n    df = add_rank(df)\n    score = calc_spread_return_sharpe(df)\n    return score\n\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n# 学習を実行する関数\ndef trainer(feature_df, feat_cols, label_col, fold_params, seed=2022, use_cache: bool = False):\n    scores = []\n    models = []\n    params = []\n    i = 0\n    for param in fold_params:\n        if not use_cache:\n            ################################\n            # データ準備\n            ################################\n            train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]\n            valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]\n\n            X_train = train[feat_cols]\n            y_train = train[label_col]\n            X_valid = valid[feat_cols]\n            y_valid = valid[label_col]\n\n            lgb_train = lgb.Dataset(X_train, y_train)\n            lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n            ################################\n            # 学習\n            ################################\n            params = {\n                'task': 'train',                   # 学習\n                'boosting_type': 'gbdt',           # GBDT\n                'objective': 'regression',         # 回帰\n                'metric': 'rmse',                  # 損失（誤差）\n                'learning_rate': 0.01,             # 学習率\n                'lambda_l1': 0.5,                  # L1正則化項の係数\n                'lambda_l2': 0.5,                  # L2正則化項の係数\n                'num_leaves': 10,                  # 最大葉枚数\n                'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n                'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n                'bagging_freq': 5,                 # バギング実施頻度\n                'min_child_samples': 10,           # 葉に含まれる最小データ数\n                'seed': seed                       # シード値\n            } \n\n            lgb_results = {}                       \n            model = lgb.train( \n                params,                            # ハイパーパラメータ\n                lgb_train,                         # 訓練データ\n                valid_sets=[lgb_train, lgb_valid], # 検証データ\n                valid_names=['Train', 'Valid'],    # データセット名前\n                num_boost_round=2000,              # 計算回数\n                early_stopping_rounds=100,         # 計算打ち切り設定\n                evals_result=lgb_results,          # 学習の履歴\n                verbose_eval=100,                  # 学習過程の表示サイクル\n            )  \n\n            ################################\n            # 結果描画\n            ################################\n            fig = plt.figure(figsize=(10, 4))\n\n            # loss\n            plt.subplot(1,2,1)\n            loss_train = lgb_results['Train']['rmse']\n            loss_test = lgb_results['Valid']['rmse']   \n            plt.xlabel('Iteration')\n            plt.ylabel('logloss')\n            plt.plot(loss_train, label='train loss')\n            plt.plot(loss_test, label='valid loss')\n            plt.legend()\n\n            # feature importance\n            plt.subplot(1,2,2)\n            importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})\n            write_df(importance, f\"importance_{i}\")\n            sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))\n\n            plt.tight_layout()\n            plt.show()\n\n            ################################\n            # 評価\n            ################################\n            # 推論\n            pred =  model.predict(X_valid, num_iteration=model.best_iteration)\n            # 評価\n            score = evaluator(valid, pred)\n            print(f\"score {i}: {score}\")\n\n            scores.append(score)\n            models.append(model)\n            # save model\n            write_model(model, f'model_{i}')\n\n        else:\n            read_model(f'model_{i}')\n        i = i + 1\n    print(\"CV_SCORES:\", scores)\n    print(\"CV_SCORE:\", np.mean(scores))\n    \n    return models","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:07:09.349369Z","iopub.execute_input":"2022-04-22T07:07:09.349618Z","iopub.status.idle":"2022-04-22T07:07:11.209628Z","shell.execute_reply.started":"2022-04-22T07:07:09.349589Z","shell.execute_reply":"2022-04-22T07:07:11.208715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2020-12-23よりも前のデータは証券コードが2000個すべて揃っていないため、これ以降のデータのみを使う。\n# (学習用データの開始日、学習用データの終了日＝検証用データの開始日、検証用データの終了日)\nfold_params = [\n    ('2020-12-23', '2021-11-01', '2021-12-01'),\n    ('2021-01-23', '2021-12-01', '2022-01-01'),\n    ('2021-02-23', '2022-01-01', '2022-02-01'),\n]\nmodels = trainer(feature_df, feat_cols, label_col, fold_params, use_cache=False)\nmodels","metadata":{"execution":{"iopub.status.busy":"2022-04-22T07:07:11.21108Z","iopub.execute_input":"2022-04-22T07:07:11.211453Z","iopub.status.idle":"2022-04-22T08:14:52.295754Z","shell.execute_reply.started":"2022-04-22T07:07:11.211419Z","shell.execute_reply":"2022-04-22T08:14:52.294567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def predictor(feature_df, feat_cols, models, is_train=True):\n    X = feature_df[feat_cols]\n    \n    # 推論\n    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))\n    print(f\"preds: {preds}\")\n    \n    # スコアは学習時のみ計算\n    if is_train:\n        scores = list(map(lambda pred: evaluator(feature_df, pred), preds))\n        print(\"SCORES:\", scores)\n\n    # 推論結果をバギング\n    pred = np.array(preds).mean(axis=0)\n\n    # スコアは学習時のみ計算\n    if is_train:\n        score = evaluator(feature_df, pred)\n        print(\"SCORE:\", score)\n    \n    return pred\n\ndef eval_predictor(df, feat_cols, models, target_date=['2022-2-01', '2022-2-27']):\n    # 日次で推論・登録\n    print(f\"target_date: {target_date}\")\n    target_df = df.copy()\n    target_df = target_df[(target_date[0] <= target_df['Date']) & (target_df['Date'] < target_date[1])]\n\n    # 推論20\n    target_df[\"pred\"] = predictor(target_df, feat_cols, models, True)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    result_df = add_rank(target_df)\n    return result_df","metadata":{"execution":{"iopub.status.busy":"2022-04-22T08:14:52.297283Z","iopub.execute_input":"2022-04-22T08:14:52.297517Z","iopub.status.idle":"2022-04-22T08:14:52.308047Z","shell.execute_reply.started":"2022-04-22T08:14:52.29749Z","shell.execute_reply":"2022-04-22T08:14:52.306911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_df = eval_predictor(feature_df, feat_cols, models)\n#pred_df = _df[0]\n#pred_df = pred_df[[\"SecuritiesCode\", \"pred\", \"Date\", \"Rank\"]]\n#target_df = _df[1]\n#target_df = target_df[[\"SecuritiesCode\", \"Target\"]]\n#result_df = pred_df.merge(target_df, on='SecuritiesCode', how=\"left\")\n#write_df(result_df, \"result_df\")\n_df","metadata":{"execution":{"iopub.status.busy":"2022-04-22T08:14:52.311416Z","iopub.execute_input":"2022-04-22T08:14:52.311809Z","iopub.status.idle":"2022-04-22T08:14:56.377583Z","shell.execute_reply.started":"2022-04-22T08:14:52.311731Z","shell.execute_reply":"2022-04-22T08:14:56.376754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"# 時系列APIのロード\nimport jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T08:14:56.378749Z","iopub.execute_input":"2022-04-22T08:14:56.378976Z","iopub.status.idle":"2022-04-22T08:14:56.405872Z","shell.execute_reply.started":"2022-04-22T08:14:56.378947Z","shell.execute_reply":"2022-04-22T08:14:56.405231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# supplemental filesを履歴データの初期状態としてセットアップ\npast_df = train_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T08:14:56.407499Z","iopub.execute_input":"2022-04-22T08:14:56.40797Z","iopub.status.idle":"2022-04-22T08:14:57.459642Z","shell.execute_reply.started":"2022-04-22T08:14:56.40792Z","shell.execute_reply":"2022-04-22T08:14:57.458746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 日次で推論・登録\nfor i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):\n    current_date = prices[\"Date\"].iloc[0]\n    print(f\"count {i}, {current_date}\")\n\n    if i == 0:\n        # リークを防止するため、時系列APIから受け取ったデータより未来のデータを削除\n        past_df = past_df[past_df[\"Date\"] < current_date]\n\n    # リソース確保のため古い履歴を削除\n    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(80))\n    past_df = past_df[past_df[\"Date\"] >= threshold]\n    \n    # 時系列APIから受け取ったデータを履歴データに統合\n    base_df = collector(prices, options, financials, trades, secondary_prices, stock_list)\n    past_df = pd.concat([past_df, base_df]).reset_index(drop=True)\n\n    # 特徴量エンジニアリング\n    feature_df, feat_cols, label_col = preprocessor(past_df, False)\n\n    # 予測対象レコードだけを抽出\n    feature_df = feature_df[feature_df['Date'] == current_date]\n\n    # 推論\n    feature_df[\"pred\"] = predictor(feature_df, feat_cols, models, False)\n\n    # 推論結果からRANKを導出し、提出データに反映\n    feature_df = add_rank(feature_df)\n    write_df(feature_df, f\"result_{i}\")\n    feature_map = feature_df.set_index('SecuritiesCode')['Rank'].to_dict()\n    sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(feature_map)\n\n    # 結果を登録\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T08:14:57.461315Z","iopub.execute_input":"2022-04-22T08:14:57.46163Z","iopub.status.idle":"2022-04-22T08:17:27.444872Z","shell.execute_reply.started":"2022-04-22T08:14:57.461588Z","shell.execute_reply":"2022-04-22T08:17:27.443258Z"},"trusted":true},"execution_count":null,"outputs":[]}]}