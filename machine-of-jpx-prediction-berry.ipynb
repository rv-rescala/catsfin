from unittest import result
import joblib
from pathlib import Path
import os
import numpy as np
import pandas as pd
from lightgbm import LGBMRegressor
import matplotlib.pyplot as plt
import warnings
from enum import Enum
from decimal import ROUND_HALF_UP, Decimal
import csv
import io
import lightgbm as lgb

warnings.filterwarnings("ignore")


PREV_MAX = 80
if os.environ.get("KAGGLE_DATA_PROXY_TOKEN") != None:
    BASE_OUTPUT_PATH = Path(f'/kaggle/working')
else:
    # for google drive
    from googleapiclient.discovery import build 
    from googleapiclient.http import MediaFileUpload 
    from oauth2client.service_account import ServiceAccountCredentials 
    from googleapiclient.http import MediaIoBaseDownload
    BASE_OUTPUT_PATH = Path(f'/tmp/kaggle/working')
BASE_INPUT_PATH = Path(f'../input/jpx-tokyo-stock-exchange-prediction')
BASE_FOLDER_ID = "1VNwdp6NEkPq5yA61PbAkoqlw0GzWKMPa"

print(f"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}")
print(f"BASE_INPUT_PATH: {BASE_INPUT_PATH}")

#class DIR_NAME(Enum):
#    train = "train_files"
#    supplemental = "supplemental_files"


def adjusting_price(df_raw, key: str):
    """[Adjusting Price/価格は単元の変化で大きく変化することがある、そのためVolumeとPriceから連続性のある値に変換する]
    Args:
        price (pd.DataFrame)  : pd.DataFrame include stock_price
    Returns:
        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose
    """

    def generate_adjusted(df):
        """
        Args:
            df (pd.DataFrame)  : stock_price for a single SecuritiesCode
        Returns:
            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode
        """
        # sort data to generate CumulativeAdjustmentFactor
        df = df.sort_values("Date", ascending=False)
        # generate CumulativeAdjustmentFactor
        df.loc[:, f"CumulativeAdjustmentFactor{key}"] = df["AdjustmentFactor"].cumprod(
        )
        # generate AdjustedClose
        df.loc[:, f"Adjusted{key}"] = (
            df[f"CumulativeAdjustmentFactor{key}"] * df[key]
        ).map(lambda x: float(
            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)
        ))
        # reverse order
        df = df.sort_values("Date")
        # to fill AdjustedClose, replace 0 into np.nan
        df.loc[df[f"Adjusted{key}"] == 0, f"Adjusted{key}"] = np.nan
        # forward fill AdjustedClose
        df.loc[:, f"Adjusted{key}"] = df.loc[:, f"Adjusted{key}"].ffill()
        return df

    # generate AdjustedClose
    df_raw = df_raw.sort_values(["SecuritiesCode", "Date"])
    df_raw = df_raw.groupby("SecuritiesCode").apply(generate_adjusted).reset_index(drop=True)

    # price.set_index("Date", inplace=True)
    return df_raw

def adjusting_volume(df_raw, key="Volume"):
    """[Adjusting Close Price/単元の変化を吸収する]
    Args:
        price (pd.DataFrame)  : pd.DataFrame include stock_price
    Returns:
        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose
    """

    def generate_adjusted(df):
        """
        Args:
            df (pd.DataFrame)  : stock_price for a single SecuritiesCode
        Returns:
            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode
        """
        # sort data to generate CumulativeAdjustmentFactor
        df = df.sort_values("Date", ascending=False)
        # generate CumulativeAdjustmentFactor
        df.loc[:, f"CumulativeAdjustmentFactor{key}"] = df["AdjustmentFactor"].cumprod(
        )
        # generate AdjustedClose
        df.loc[:, f"Adjusted{key}"] = (
            df[key] / df[f"CumulativeAdjustmentFactor{key}"]
        ).map(lambda x: float(
            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)
        ))
        # reverse order
        df = df.sort_values("Date")
        # to fill AdjustedClose, replace 0 into np.nan
        df.loc[df[f"Adjusted{key}"] == 0, f"Adjusted{key}"] = np.nan
        # forward fill AdjustedClose
        df.loc[:, f"Adjusted{key}"] = df.loc[:, f"Adjusted{key}"].ffill()
        return df

    # generate AdjustedClose
    df_raw = df_raw.sort_values(["SecuritiesCode", "Date"])
    df_raw = df_raw.groupby("SecuritiesCode").apply(
        generate_adjusted).reset_index(drop=True)

    # price.set_index("Date", inplace=True)
    return df_raw

def read_prices(dir_name: str, securities_codes = None):
    """[Important: the dateset of 2020/10/1 is lost because of system failer in JPX, see: https://www.jpx.co.jp/corporate/news/news-releases/0060/20201019-01.html]

    """
    path = BASE_INPUT_PATH / f"{dir_name}/stock_prices.csv"
    print(f"read_prices: {path}")
    df = pd.read_csv(path)
    df = df[df['Open'].notna()]
    if securities_codes:
        df = df[df['SecuritiesCode'].apply(lambda x: x==securities_codes)]
    return df

def read_stock_list(securities_codes = None, only_universe: bool = True):
    """_summary_

    Args:
        securities_codes (_type_, optional): _description_. Defaults to None.
        only_universe (bool, optional): _description_. Defaults to True.

    Returns:
        _type_: _description_
    """
    path = BASE_INPUT_PATH / 'stock_list.csv'
    print(f"read_stock_list: {path}")
    df = pd.read_csv(path)
    if only_universe:
        df = df[df['Universe0']]
    if securities_codes:
        df = df[df['SecuritiesCode'].apply(lambda x: x==securities_codes)]
    return df

def read_financials(dir_name: str, securities_codes = None):
    """_summary_

    Args:
        dir_name (str): _description_
        securities_codes (_type_, optional): _description_. Defaults to None.

    Returns:
        _type_: _description_
    """
    path = BASE_INPUT_PATH / f"{dir_name}/financials.csv"
    print(f"read_financials: {path}")
    df = pd.read_csv(path)
    if securities_codes:
        df = df[df['SecuritiesCode'].apply(lambda x: x==securities_codes)]
    return df

def merge_data_by_price(prices, stock_list, financials = None):
    """[処理ベースとなるデータセットを作成]

    Args:
        prices (_type_): _description_
        stock_list (_type_): _description_

    Returns:
        _type_: _description_
    """
    # stock_prices がベース
    base_df = prices.copy()

    # stock_listと結合
    stock_list.rename(columns={'Close': 'Close_x'}, inplace=True)
    base_df = base_df.merge(stock_list, on='SecuritiesCode', how="left")
    
    if financials:
        base_df = base_df.merge(financials, on=['SecuritiesCode', 'Date'], how="left")
    return base_df

def read_submission_test():
    """
    api iteratorは一度しか実行されないため、submissionからデータを読み取る
    # prices, options, financials, trades, secondary_prices, sample_prediction
    # Targetはtrain/pricesにあるため、train_filesから取得する
    """
    prices = pd.read_csv(
        BASE_INPUT_PATH / 'supplemental_files/stock_prices.csv')
    options = pd.read_csv(BASE_INPUT_PATH / 'example_test_files/options.csv')
    financials = pd.read_csv(
        BASE_INPUT_PATH / 'example_test_files/financials.csv')
    trades = pd.read_csv(BASE_INPUT_PATH / 'example_test_files/trades.csv')
    secondary_prices = pd.read_csv(
        BASE_INPUT_PATH / 'example_test_files/secondary_stock_prices.csv')
    sample_prediction = pd.read_csv(
        BASE_INPUT_PATH / 'example_test_files/sample_submission.csv')
    dates = options["Date"].unique()
    print(f"submission target date: {dates}")
    for d in dates:
        target_prices = prices[prices["Date"] == d]
        target_options = options[options["Date"] == d]
        target_financials = financials[financials["Date"] == d]
        target_trades = trades[trades["Date"] == d]
        target_secondary_prices = secondary_prices[secondary_prices["Date"] == d]
        target_sample_prediction = sample_prediction[sample_prediction["Date"] == d]
        yield target_prices, target_options, target_financials, target_trades, target_secondary_prices, target_sample_prediction
        
def read_eval_test(eval_dates: None):
    """
    test 
    """
    train_folder = "train_files"
    supplemental_folder = "supplemental_files"
    prices_t = pd.read_csv(BASE_INPUT_PATH / f'{train_folder}/stock_prices.csv')
    prices_o = pd.read_csv(BASE_INPUT_PATH / f'{supplemental_folder}/stock_prices.csv')
    prices = pd.concat([prices_t, prices_o]).reset_index(drop=True)
    
    options_t = pd.read_csv(BASE_INPUT_PATH / f'{train_folder}/options.csv')
    options_o = pd.read_csv(BASE_INPUT_PATH / f'{supplemental_folder}/options.csv')
    options = pd.concat([options_t, options_o]).reset_index(drop=True)
    
    financials_t = pd.read_csv(BASE_INPUT_PATH / f'{train_folder}/financials.csv')
    financials_o = pd.read_csv(BASE_INPUT_PATH / f'{supplemental_folder}/financials.csv')
    financials = pd.concat([financials_t, financials_o]).reset_index(drop=True)
    
    trades_t = pd.read_csv(BASE_INPUT_PATH / f'{train_folder}/trades.csv')
    trades_o = pd.read_csv(BASE_INPUT_PATH / f'{supplemental_folder}/trades.csv')
    trades = pd.concat([trades_t, trades_o]).reset_index(drop=True)
    
    secondary_prices_t = pd.read_csv(BASE_INPUT_PATH / f'{train_folder}/secondary_stock_prices.csv')
    secondary_prices_o = pd.read_csv(BASE_INPUT_PATH / f'{supplemental_folder}/secondary_stock_prices.csv')
    secondary_prices = pd.concat([secondary_prices_t, secondary_prices_o]).reset_index(drop=True)
    
    # submission
    sample_prediction = pd.read_csv(BASE_INPUT_PATH / 'example_test_files/sample_submission.csv')

    for d in eval_dates:
        target_prices = prices[prices["Date"] == d]
        target_options = options[options["Date"] == d]
        target_financials = financials[financials["Date"] == d]
        target_trades = trades[trades["Date"] == d]
        target_secondary_prices = secondary_prices[secondary_prices["Date"] == d]
        target_sample_prediction = sample_prediction[sample_prediction["Date"] == d]
        yield target_prices, target_options, target_financials, target_trades, target_secondary_prices, target_sample_prediction

def read_train_data(securities_codes = None, with_supplemental: bool = True, cache_name = None):
    """[The train base is price dataset, the other data are joined to prices DF by left join]

    """
    if cache_name:
        return pd.read_csv(BASE_OUTPUT_PATH / f'{cache_name}.csv')

    # origin
    df = merge_data_by_price(prices=read_prices(dir_name="train_files", securities_codes=securities_codes),
                    stock_list=read_stock_list(securities_codes=securities_codes))

    # supplyment
    if with_supplemental:
        supplemental_df = merge_data_by_price(prices=read_prices(
            dir_name="supplemental_files", securities_codes=securities_codes), stock_list=read_stock_list(securities_codes=securities_codes))
        df = pd.concat([df, supplemental_df]).reset_index(drop=True)

    # historical
    df = adjusting_price(df, "Close")
    df = adjusting_price(df, "Open")
    df = adjusting_price(df, "High")
    df = adjusting_price(df, "Low")
    df = adjusting_volume(df)
    
    # format
    df.loc[:, "Date"] = pd.to_datetime(df.loc[:, "Date"], format="%Y-%m-%d")
    df.loc[:, "EffectiveDate"] = pd.to_datetime(df.loc[:, "EffectiveDate"], format="%Y%m%d")
    
    dates = df["Date"].unique()
    print(f"read_train_data target date: {dates}")
    return df


def create_inference_data(additional_prices, securities_codes = None, target_feat_cols = None):
    """[推論フェーズで使用するデータ]

    Args:
        additional_prices (_type_): 追加されたprice
        stock_list (_type_): _description_

    Returns:
        _type_: _description_
    """
    current_date = additional_prices["Date"].iloc[0]
    print(f"create_inference_data: {current_date}")
    stock_list = read_stock_list(securities_codes=securities_codes)
    # prev file
    df_train = merge_data_by_price(prices=read_prices(dir_name="train_files", securities_codes=securities_codes), stock_list=stock_list)
    df_train = df_train[df_train["Date"] < current_date]
    #print(f"train dates max: {df_train['Date'].unique()}")
    
    df_supplemental = merge_data_by_price(prices=read_prices(dir_name="supplemental_files", securities_codes=securities_codes), stock_list=stock_list)
    df_supplemental = df_supplemental[df_supplemental["Date"] < current_date]
    #print(f"sup dates max: {df_supplemental['Date'].unique()}")
    
    df_additionl = merge_data_by_price(prices=additional_prices, stock_list=stock_list)

    # join
    df_prev = pd.concat([df_train, df_supplemental]).reset_index(drop=True)
    df = pd.concat([df_additionl, df_prev]).reset_index(drop=True)
    
    # 古い履歴を削除
    MAX_DURATION = 80
    threshold = (pd.Timestamp(current_date) - pd.offsets.BDay(MAX_DURATION)).strftime("%Y-%m-%d")
    df = df[df["Date"] >= threshold]
    df = df.sort_values("Date", ascending=False)
    print(f"dates: {df['Date'].unique()}")
    
    # AdjustedClose項目の生成
    df = adjusting_price(df, "Close")
    df = adjusting_price(df, "Open")
    df = adjusting_price(df, "High")
    df = adjusting_price(df, "Low")
    df = adjusting_volume(df)
    
    # feature
    df = add_features(df=df, is_train=False, target_feat_cols=target_feat_cols)[0]

    # format
    target_df = df[df["Date"] == current_date]
    target_df.loc[:, "Date"] = pd.to_datetime(target_df.loc[:, "Date"], format="%Y-%m-%d")
    print(f"create_inference_data, df length is {target_df.index}")
    return target_df

# for feature
def cal_moving_average(key:str, periods):
    def func(df):
        for period in periods:
            col = f"MovingAverage{key}{period}"
            col_gap = f"{col}GapPercent"
            df[col] = df[key].rolling(period, min_periods=1).mean()
            df[col_gap] = (df[key] / df[col]) * 100.0
        return df
    return func

def cal_changing_ration(key:str, periods):
    def func(df):
        for period in periods:
            col = f"ChangingRatio{key}{period}"
            df[col] = df[key].pct_change(period) * 100
        return df
    return func

def cal_historical_vix(key: str, periods):
    def func(df):
        for period in periods:
            col = f"HistoricalVIX{key}{period}"
            df[col] = np.log(df[key]).diff().rolling(period).std()
        return df
    return func

def add_columns_per_code(df, functions):
    def func(df):
        for f in functions:
            df = f(df)
        return df
    df = df.sort_values(["SecuritiesCode", "Date"])
    df = df.groupby("SecuritiesCode").apply(func)
    df = df.reset_index(drop=True)
    return df

def add_columns_per_day(base_df):
    base_df['diff_rate1'] = (base_df['Close'] - base_df['Open']) / base_df['Close']
    base_df['diff_rate2'] = (base_df['High'] - base_df['Low']) / base_df['Close']    
    return base_df

def generate_features(df):
    base_df = df.copy()
    prev_column_names = base_df.columns
    periods = [5, 25, 75]
    functions = [
        cal_moving_average("AdjustedClose", periods),
        cal_moving_average("AdjustedOpen", periods),
        cal_moving_average("AdjustedHigh", periods),
        cal_moving_average("AdjustedLow", periods),
        cal_moving_average("AdjustedVolume", periods),
        cal_changing_ration("AdjustedClose", periods),
        cal_changing_ration("AdjustedOpen", periods),
        cal_changing_ration("AdjustedHigh", periods),
        cal_changing_ration("AdjustedLow", periods),
        cal_changing_ration("AdjustedVolume", periods),
        cal_historical_vix("AdjustedClose", periods),
        cal_historical_vix("AdjustedOpen", periods),
        cal_historical_vix("AdjustedHigh", periods),
        cal_historical_vix("AdjustedLow", periods),
        cal_historical_vix("AdjustedVolume", periods)
    ]
    
    base_df = add_columns_per_code(base_df, functions)
    base_df = add_columns_per_day(base_df)
    
    add_column_names = list(set(base_df.columns) - set(prev_column_names))
    base_df = base_df[base_df["HistoricalVIXAdjustedClose75"] != 0]
    return base_df, add_column_names

def select_features(feature_df, add_column_names, is_train):
    base_cols = ['RowId', 'Date', 'SecuritiesCode']
    numerical_cols = sorted(add_column_names)
    categorical_cols = ['NewMarketSegment', '33SectorCode', '17SectorCode']
    label_col = ['Target']
    feat_cols = numerical_cols + categorical_cols
    feature_df = feature_df[base_cols + feat_cols + label_col]
    feature_df[categorical_cols] = feature_df[categorical_cols].astype('category')
    if is_train:
        feature_df.dropna(inplace=True)
    else:
        feature_df[numerical_cols] = feature_df[numerical_cols].fillna(0)
        feature_df[numerical_cols] = feature_df[numerical_cols].replace([np.inf, -np.inf], 0)
    return feature_df, feat_cols, label_col


def add_features(df, is_train=True, target_feat_cols=None):
    """_summary_

    Args:
        df (_type_): _description_
        is_train (bool, optional): _description_. Defaults to True.
        target_feat_cols (_type_, optional): _description_. Defaults to None.

    Returns:
        _type_: _description_
    """
    feature_df = df.copy()
    print(feature_df.columns)
    
    ## 特徴量生成
    feature_df, add_column_names = generate_features(feature_df)
    
    ## 特徴量選択
    feature_df, feat_cols, label_col = select_features(feature_df, add_column_names, is_train)
    
    if target_feat_cols:
        feat_cols = target_feat_cols
    return feature_df, feat_cols, label_col

# for eval
def add_rank(df, col_name="Prediction"):
    df["Rank"] = df.groupby("Date")[col_name].rank(
        ascending=False, method="first") - 1
    df["Rank"] = df["Rank"].astype("int")
    return df

def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size: int = 200, toprank_weight_ratio: float = 2) -> float:
    """
    Args:
        df (pd.DataFrame): predicted results
        portfolio_size (int): # of equities to buy/sell
        toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.
    Returns:
        (float): sharpe ratio
    """
    def _calc_spread_return_per_day(df, portfolio_size, toprank_weight_ratio):
        """
        Args:
            df (pd.DataFrame): predicted results
            portfolio_size (int): # of equities to buy/sell
            toprank_weight_ratio (float): the relative weight of the most highly ranked stock compared to the least.
        Returns:
            (float): spread return
        """
        #print(f"date: {df['Date'].min()}")
        #print(f"min: {df['Rank'].min()}")
        #print(f"max: {df['Rank'].max()}")
        assert df['Rank'].min() == 0
        assert df['Rank'].max() == len(df['Rank']) - 1
        weights = np.linspace(start=toprank_weight_ratio,
                              stop=1, num=portfolio_size)
        purchase = (df.sort_values(by='Rank')[
                    'Target'][:portfolio_size] * weights).sum() / weights.mean()
        short = (df.sort_values(by='Rank', ascending=False)[
                 'Target'][:portfolio_size] * weights).sum() / weights.mean()
        #print(f"purchase: {purchase}")
        #print(f"short: {short}")
        return purchase - short

    print("--------------------------")
    buf = df.groupby('Date').apply(_calc_spread_return_per_day,
                                   portfolio_size, toprank_weight_ratio)
    print(f"mean: {buf.mean()}")
    print(f"std: {buf.std()}")
    mean = buf.mean() 
    std = buf.std()
    sharpe_ratio = buf.mean() / buf.std()
    print(f"sharpe_ratio: {sharpe_ratio}")
    print("--------------------------")
    return sharpe_ratio, mean, std

# 予測用のデータフレームと、予測結果をもとに、スコアを計算する関数
def evaluator(df, pred):
    df["Prediction"] = pred
    df = add_rank(df)
    score, mean, std = calc_spread_return_sharpe(df)
    return score, mean, std

def eval_predictor(l_df):
    df = pd.concat(l_df)
    score, mean, std  = calc_spread_return_sharpe(df)
    print(f"eval_predictor scoure : {score}, {mean}, {std}")
    return score, mean, std 

def predictor(feature_df, feat_cols, models):
    """
    is_train = Trueの場合、Dateが複数日にまたがらないと正しいが計算されない(標準偏差が0になる)
    """
    X = feature_df[feat_cols]

    print(models)
    print(f"model len: {len(models)}")
    print(f"X dtypes: {X.dtypes}")
    
    # 推論/List[model数][X]
    preds = list(map(lambda model: model.predict(X, num_iteration=model.best_iteration), models))
    pred = np.array(preds).mean(axis=0)
    return pred, preds

def write_df(df, name, upload:bool = False, formats = ["pickle"]):
    """_summary_

    Args:
        df (_type_): _description_
        filename (_type_): _description_
    """
    
    for format in formats:
        filename = f"{name}.{format}"
        path = f'{BASE_OUTPUT_PATH}/{filename}'
        print(f"write_df: {path}")
        if format == "csv":
            df.to_csv(path, index=False)
        elif format == "pickle":
            df.to_pickle(path)
        if upload and (format == "csv"):
            gdrive = GoogleDrive(key_file_path =  f"{BASE_INPUT_PATH}/gdrive.json", base_folder_id = BASE_FOLDER_ID)
            gdrive.upload_file(filename=filename, local_path=str(path))
    return path

def read_df(cache_name: str):
    fullpath = BASE_OUTPUT_PATH / f'{cache_name}.pickle'
    print(f"read_df: {fullpath}")
    # pd.read_csv()
    df = pd.read_pickle(fullpath)
    return df

def write_model(model, name, upload: bool = False):
    """[write trained model]

    Args:
        model (_type_): _description_
        name (_type_): _description_
    """
    # save model
    model_name = f"{name}_model.mdl"
    path = f'{BASE_OUTPUT_PATH}/{model_name}'
    print(f"write_model: {path}")
    #joblib.dump(model, path)
    model.save_model(path)
    if upload:
        gdrive = GoogleDrive(key_file_path =  f"{BASE_INPUT_PATH}/gdrive.json", base_folder_id = BASE_FOLDER_ID)
        gdrive.upload_file(filename=model_name, local_path=str(path))

def write_models(models, name, upload: bool = False):
    """[write trained model]

    Args:
        model (_type_): _description_
        name (_type_): _description_
    """
    # save model
    i = 0
    for model in models:
        write_model(model=model, name=f"{name}_model_{i}", upload=upload)
        i = i + 1


# load model
def read_models(name, download: bool = False):
    """[read trained model]

    Args:
        name (_type_): _description_

    Returns:
        _type_: _description_
    """
    base_model_name = f'{name}_model'
    print(f"read_models: {base_model_name}")
    if download:
        gdrive = GoogleDrive(key_file_path =  f"{BASE_INPUT_PATH}/gdrive.json", base_folder_id = BASE_FOLDER_ID)
        paths = gdrive.downloads(base_model_name)
    else:
        paths = list(filter(lambda x: base_model_name in x, os.listdir(path=f"{BASE_OUTPUT_PATH}")))
    print(f"paths: {paths}")
    results = []
    for path in paths:
        results.append(lgb.Booster(model_file=f"{BASE_OUTPUT_PATH}/{path}"))
        # yield joblib.load(f"{BASE_OUTPUT_PATH}/{path}")
    return results

def write_scores(scores, name, upload: bool = False):
    filename = f"{name}_scores.csv"
    save_path = BASE_OUTPUT_PATH / filename
    with open(save_path, 'w') as f:
        # using csv.writer method from CSV package
        for s in scores:
            f.write(s + '\n')
    if upload:
        gdrive = GoogleDrive(key_file_path =  f"{BASE_INPUT_PATH}/gdrive.json", base_folder_id = BASE_FOLDER_ID)
        print(str(save_path))
        gdrive.upload_file(filename=filename, local_path=str(save_path))

class GoogleDrive():
    """[summary]
    """
    def __init__(self, key_file_path: str, base_folder_id: str):
        self.service = self.get_google_service(key_file_path=key_file_path)
        self.base_folder_id = base_folder_id

    def get_google_service(self, key_file_path: str):
        scope = ['https://www.googleapis.com/auth/drive.file'] 
        credentials = ServiceAccountCredentials.from_json_keyfile_name(key_file_path, scopes=scope)
        return build("drive", "v3", credentials=credentials, cache_discovery=False) 

    def upload_file(self, filename:str, local_path:str, parents:str=None):
        # delete first
        keys = self.list_key_id()
        for key in keys:
            if filename == key["name"]:
                self.delete(key["id"])
        
        ext = os.path.splitext(local_path.lower())[1][1:]
        print(ext)
        if not parents:
            parents = self.base_folder_id
        m = 'application/octet-stream'
        print(parents)
        file_metadata = {"name": filename, "parents": [parents] } 
        media = MediaFileUpload(local_path, m,resumable=True)
        file = self.service.files().create(body=file_metadata, media_body=media, fields='id').execute()
        return file.get('id')

    def create_folder(self, folder_name: str, parents: str=None):
        if not parents:
            parents = self.base_folder_id
        file_metadata = {
            'name': folder_name,
            'mimeType': "*/*",
            "parents": [parents]
        }
        file = self.service.files().create(body=file_metadata, fields='id').execute()
        return file.get('id')

    def delete(self, id):
        self.service.files().delete(fileId=id).execute()

    def downloads(self, str_contain: str):
        keys = self.list_key_id()
        r = []
        for key in keys:
            if str_contain in key["name"]:
                request = self.service.files().get_media(fileId=key["id"])
                path = BASE_OUTPUT_PATH / key["name"]
                fh = io.FileIO(path, mode='wb')
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while done is False:
                    status, done = downloader.next_chunk()
            r.append(path)
        return r

    def list_key_id(self):
        page_token = None
        fl = []
        while True:
            response = self.service.files().list(
                                                spaces='drive',
                                                fields='nextPageToken, files(id, name)',
                                                pageToken=page_token).execute()
            for file in response.get('files', []):
                # Process change
                print(file.get('name') + "," + file.get('id'))
                fl.append({'name': file.get('name'), 'id': file.get('id')})
            page_token = response.get('nextPageToken', None)
            if page_token is None:
                break
        return fl

    def delete_all(self):
        for k in self.list_key_id():
            self.delete(k["id"])

            
# %% [code]
# 
import os
from pathlib import Path
from decimal import ROUND_HALF_UP, Decimal

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error

import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb

import warnings
warnings.simplefilter('ignore')
APP = "berry"
train_file = f"train_{APP}"
feature_file = f"feature_{APP}"
eval_feature_file = f"eval_feature_{APP}"

if os.environ.get("KAGGLE_DATA_PROXY_TOKEN") == None:
    from func import *

UPLOAD = False
TRAIN_INIT = False
FEATURE_INIT = False
MODEL_INIT = False
EVANL_INIT = False
EVAL_MODEL = False
SUBMIT = True


target_feat_cols = ['33SectorCode', 'ChangingRatioAdjustedVolume25']

if TRAIN_INIT or SUBMIT:
    df_stock = read_stock_list()
    df_train = read_train_data()
    if not SUBMIT:
        write_df(df_train, train_file)
else:
    df_train = read_df(train_file)

if FEATURE_INIT or SUBMIT:
    feature_df, feat_cols, label_col = add_features(df=df_train, target_feat_cols=target_feat_cols)
    if not SUBMIT:
        write_df(feature_df, feature_file)
else:
    feature_df = read_df(feature_file)
    feat_cols = target_feat_cols
    label_col = "Target"
    
print(f"feat_cols: {feat_cols}")

# %%
# 学習を実行する関数
def trainer(feature_df, feat_cols, label_col, fold_params, seed=2022):
    scores = []
    models = []
    params = []
    scores_str = []
    i = 0
    for param in fold_params:
        ################################
        # データ準備
        ################################
        print(f"feature_df date: {feature_df['Date'].unique()}")
        
        train_duration = f"{param[0]}-{param[1]}"
        valid_duration = f"{param[1]}-{param[2]}"
        train = feature_df[(param[0] <= feature_df['Date']) & (feature_df['Date'] < param[1])]
        valid = feature_df[(param[1] <= feature_df['Date']) & (feature_df['Date'] < param[2])]
        
        print(f"train_duration date: {train['Date'].unique()}")
        print(f"valid_duration date: {valid['Date'].unique()}")
        print(f"train: {train.index}")
        print(f"valid: {valid.index}")

        X_train = train[feat_cols]
        y_train = train[label_col]
        X_valid = valid[feat_cols]
        y_valid = valid[label_col]

        lgb_train = lgb.Dataset(X_train, y_train)
        lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)

        ################################
        # 学習
        ################################
        params = {
            'task': 'train',                   # 学習
            'boosting_type': 'gbdt',           # GBDT
            'objective': 'regression',         # 回帰
            'metric': 'rmse',                  # 損失（誤差）
            'learning_rate': 0.01,             # 学習率
            'lambda_l1': 0.5,                  # L1正則化項の係数
            'lambda_l2': 0.5,                  # L2正則化項の係数
            'num_leaves': 10,                  # 最大葉枚数
            'feature_fraction': 0.5,           # ランダムに抽出される列の割合
            'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合
            'bagging_freq': 5,                 # バギング実施頻度
            'min_child_samples': 10,           # 葉に含まれる最小データ数
            'seed': seed                       # シード値
        } 

        lgb_results = {}                       
        model = lgb.train( 
            params,                            # ハイパーパラメータ
            lgb_train,                         # 訓練データ
            valid_sets=[lgb_train, lgb_valid], # 検証データ
            valid_names=['Train', 'Valid'],    # データセット名前
            num_boost_round=2000,              # 計算回数
            early_stopping_rounds=100,         # 計算打ち切り設定
            evals_result=lgb_results,          # 学習の履歴
            verbose_eval=100,                  # 学習過程の表示サイクル
        )  

        ################################
        # 結果描画
        ################################
        fig = plt.figure(figsize=(10, 4))

        # loss
        plt.subplot(1,2,1)
        loss_train = lgb_results['Train']['rmse']
        loss_test = lgb_results['Valid']['rmse']   
        plt.xlabel('Iteration')
        plt.ylabel('logloss')
        plt.plot(loss_train, label='train loss')
        plt.plot(loss_test, label='valid loss')
        plt.legend()

        # feature importance
        plt.subplot(1,2,2)
        importance = pd.DataFrame({'feature':feat_cols, 'importance':model.feature_importance()})
        if not SUBMIT:
            write_df(importance, f"importance_{APP}_{i}")
        sns.barplot(x = 'importance', y = 'feature', data = importance.sort_values('importance', ascending=False))

        plt.tight_layout()
        plt.show()

        ################################
        # 評価
        ################################
        # 推論
        pred =  model.predict(X_valid, num_iteration=model.best_iteration)
        # 評価
        score, mean, std = evaluator(valid, pred)

        scores.append(score)
        models.append(model)
        
        scores_str.append(f"{train_duration},{valid_duration},{score},{mean},{std}")
        i = i + 1
    print("CV_SCORES:", scores)
    print("CV_SCORE:", np.mean(scores))
    return models, scores_str


# %%

fold_params = [
    ('2020-12-23', '2021-11-01', '2021-12-01'),
    ('2021-01-23', '2021-12-01', '2022-01-01'),
    ('2021-02-23', '2022-01-01', '2022-02-01'),
]

if MODEL_INIT or SUBMIT:
    models, scores = trainer(feature_df, feat_cols, label_col, fold_params)
    print(scores)
    if not SUBMIT:
        write_models(models=models, name=APP, upload=UPLOAD)
        write_scores(scores=scores, name=f"train_{APP}", upload=UPLOAD)
else:
    models = read_models(f"{APP}")    

# %%
# モデルの評価
if EVAL_MODEL or not SUBMIT:
    eval_dates = ['2021-11-04', '2021-11-05', '2021-11-06']
    total_scours = []
    pred_results = []
    for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(read_eval_test(eval_dates)):
        if prices.empty:
            print('prices is empty!')
            continue
        current_date = prices["Date"].iloc[0]
        print(f"count {i}, {current_date}")
        print(feat_cols)
        
        # 時系列APIから受け取ったデータを履歴データに統合
        if EVANL_INIT:
            df_inf = create_inference_data(additional_prices=prices, target_feat_cols=target_feat_cols)
            write_df(df_inf, eval_feature_file + current_date)
        else:
            df_inf = read_df(eval_feature_file + current_date)

        # 推論
        pred, preds = predictor(feature_df=df_inf, feat_cols=feat_cols, models=models)
        df_inf["Prediction"] = pred
        
        # result with rank
        df_inf = add_rank(df_inf)
        write_df(df_inf, f"eval_result_{i}_{current_date}")
        pred_results.append(df_inf)
    scores, mean, std = eval_predictor(pred_results)
    eval_dates_str = "_".join(eval_dates)
    write_scores([f"{eval_dates_str},{scores},{mean},{std}"], f"eval_{APP}_{eval_dates_str}")

if SUBMIT:
    import jpx_tokyo_market_prediction
    env = jpx_tokyo_market_prediction.make_env()
    iter_test = env.iter_test()

    # %% [code] {"execution":{"iopub.status.busy":"2022-04-22T11:31:08.220712Z","iopub.execute_input":"2022-04-22T11:31:08.220948Z","iopub.status.idle":"2022-04-22T11:35:41.755885Z","shell.execute_reply.started":"2022-04-22T11:31:08.220921Z","shell.execute_reply":"2022-04-22T11:35:41.755004Z"}}
    # 日次で推論・登録
    for i, (prices, options, financials, trades, secondary_prices, sample_prediction) in enumerate(iter_test):
        if prices.empty:
            print('prices is empty!')
            continue
        current_date = prices["Date"].iloc[0]
        print(f"count {i}, {current_date}")
        print(feat_cols)
        
        # 時系列APIから受け取ったデータを履歴データに統合
        df_inf = create_inference_data(additional_prices=prices, target_feat_cols=target_feat_cols)

        # 推論
        pred, preds = predictor(feature_df=df_inf, feat_cols=feat_cols, models=models)
        df_inf["Prediction"] = pred
        
        # result with rank
        df_inf = add_rank(df_inf)
        write_df(df_inf, f"submit_result_{i}_{current_date}", formats=["csv"])
        df_inf = df_inf.set_index('SecuritiesCode')['Rank'].to_dict()
        sample_prediction['Rank'] = sample_prediction['SecuritiesCode'].map(df_inf)

        # 結果を登録
        env.predict(sample_prediction)